# Snakemake Workflow for Gesenius Data 
# see https://snakemake.readthedocs.io

from pathlib import Path

tf_modules = [
    # order strategically to set priorities for features
    "/Users/cody/github/CambridgeSemiticsLab/time_collocations/data/bhsa/tf", 
    "/Users/cody/github/etcbc/bhsa/tf/c", 
    "/Users/cody/github/etcbc/genre_synvar/tf/c", 
    "/Users/cody/github/etcbc/valence/tf/c",
    "/Users/cody/github/etcbc/heads/tf/c",
]

# -- clone necessary data -- 

# Need to complete the gitclone rule begun below
# see: https://github.com/snakemake/snakemake/issues/350
# rule get_bhsa:
#     output:
#         directory(expand("~/github/{package}", package=bhsa_packages))
#     shell:
#         """
#         if [[! -d "{wildcards.package}" ]]; then
#             git clone https://github.com/{}    
#         """

# -- generate samples --
rule get_samples:
    input:
        tf_mod="/Users/cody/github/etcbc/bhsa/tf/c",
        script="scripts/bhsa/get_samples.py",
    output:
        file="../results/samples/{verb}.json" 
    script:
        "scripts/bhsa/get_samples.py"

# -- preprocess the raw GBI data --
rule preprocess_gbi:
    input:
        niv="_private_/GBI_alignment/niv84.ot.alignment.json",
        esv="_private_/GBI_alignment/esv.ot.alignment.json",    
    output:
        word_data="_private_/GBI_alignment/word_data.pickle",
        verse2words="_private_/GBI_alignment/verse2words.pickle",
        linkbyid="_private_/GBI_alignment/linkbyid.pickle",
        id2link="_private_/GBI_alignment/id2link.pickle",
    script: 
        "scripts/english/parse/preprocess_gbi.py"

# -- prepare GBI verb data for Spacy parsing --
# first match 1 BHSA verb node to 1 GBI word id
# then match the GBI word data to translation data
rule prepare_gbi_verbs:
    input:
        script="scripts/english/parse/prepare_verbs.py",
        bhsa2gbi="_private_/GBI_alignment/bhsa2gbi.json",
        tf_mods=tf_modules,
        word_data=rules.preprocess_gbi.output.word_data,
        id2link=rules.preprocess_gbi.output.id2link,
    output:
        matches="_private_/GBI_alignment/verb_bhsa2gbi.pickle",
        no_match="_private_/GBI_alignment/verb_no_match.txt",
        eng_verbs="_private_/GBI_alignment/english_verbs.pickle",
        versestoparse="_private_/GBI_alignment/versestoparse.pickle",
    script:
        "scripts/english/parse/prepare_verbs.py"

# -- automatically parse english data --
rule parse_eng_verses:
    input:
        versestoparse=rules.prepare_gbi_verbs.output.versestoparse,
        verse2words=rules.preprocess_gbi.output.verse2words,
        word_data=rules.preprocess_gbi.output.word_data,
        script="scripts/english/parse/parse_verses.py",
    output:
        parsedverses="_private_/GBI_alignment/parsings/parsedverses.pickle",
        verse2text="_private_/GBI_alignment/verse2text.json",
    script:
        "scripts/english/parse/parse_verses.py"

# -- apply automatic matches to parsed english data to get tense tags
rule parse_eng_tenses:
    input:
        main_script="scripts/english/parse/parse_tenses.py",
        impv_script="scripts/english/parse/imperatives.py",
        tense_rules="scripts/english/parse/tense_rules.py",
        parsedverses=rules.parse_eng_verses.output.parsedverses,
        eng_verbs=rules.prepare_gbi_verbs.output.eng_verbs,
        word_data=rules.preprocess_gbi.output.word_data,
        verse2words=rules.preprocess_gbi.output.verse2words,
    output:
        esv="_private_/GBI_alignment/parsings/bhsa2esv.json",
        niv="_private_/GBI_alignment/parsings/bhsa2niv.json",
        inspect_esv="_private_/GBI_alignment/parsings/inspect_esv_tenses.txt",
        inspect_niv="_private_/GBI_alignment/parsings/inspect_niv_tenses.txt",
    script:
        "scripts/english/parse/parse_tenses.py"

# -- apply manual corrections --
# How to work in manual corrections to the pipeline?
# We do it in a round-about way by keeping some outputs
# secret from snakemake and by designing dummy outputs.

esv_corr = "../results/datasets/yqtl/corr_esv_{verb}.json"
niv_corr = "../results/datasets/yqtl/corr_niv_{verb}.json"

# produce initial to-do files
rule get_eng_corrections:
    input:
        script="scripts/english/parse/get_corrections.py",
        script2="scripts/english/parse/correction_files.py",
        verse2text=rules.parse_eng_verses.output.verse2text,
        esv=rules.parse_eng_tenses.output.esv,
        niv=rules.parse_eng_tenses.output.niv,
        sample=rules.get_samples.output.file, 
    params:
        esv_corr=esv_corr,
        niv_corr=niv_corr,
    output:
        esv_todo="_private_/GBI_alignment/parsings/corrections/esv_{verb}.todo",
        niv_todo="_private_/GBI_alignment/parsings/corrections/niv_{verb}.todo",
    script:
        "scripts/english/parse/get_corrections.py"

# every time to-do file is changed, this rule will run;
# a dummy file "run_{verb}" is used as the target, but 
# the true output is another copy of the corrections and 
# to-do files, which have been updated to reflect the latest
# state of the to-do files
rule apply_eng_corrections:
    input:
        script="scripts/english/parse/apply_corrections.py",
        script2="scripts/english/parse/correction_files.py",
        verse2text=rules.parse_eng_verses.output.verse2text,
        esv=rules.parse_eng_tenses.output.esv,
        niv=rules.parse_eng_tenses.output.niv,
        esv_todo=rules.get_eng_corrections.output.esv_todo, # changes to to-dos trigger run
        niv_todo=rules.get_eng_corrections.output.niv_todo,
    params:
        niv_corr=niv_corr,
        esv_corr=esv_corr,
    output:
        run="_private_/GBI_alignment/parsings/corrections/run_{verb}" # blank dummy file to make pipeline run
    script:
        "scripts/english/parse/apply_corrections.py"

# -- build csv tables --

# - BHSA -
bhsa_scripts = Path("scripts/bhsa")
main_bhsa="scripts/bhsa/build_bhsa_tables.py"
rule bhsa_tables:
    input:
        sample=rules.get_samples.output.file, 
        tf_mods=tf_modules,
        scripts=list(bhsa_scripts.glob('*.py'))
    output:
        bhsa="../results/datasets/{verb}/bhsa.csv",
        bhsa_clrela="../results/datasets/{verb}/bhsa_clrela.csv",
    script:
        main_bhsa

# - English -
eng_scripts=Path("scripts/english")
rule eng_tables:
    input:
        sample=rules.get_samples.output.file, 
        esv=rules.parse_eng_tenses.output.esv,
        niv=rules.parse_eng_tenses.output.niv,
        niv_corr=niv_corr,
        esv_corr=esv_corr,
        #txt=rules.parse_eng_verses.output.verse2text,
        parsedverses=rules.parse_eng_verses.output.parsedverses,
        scripts=list(eng_scripts.glob('*.py')),
    output: 
        eng="../results/datasets/{verb}/eng.csv",
        eng_text="../results/datasets/{verb}/eng_text.csv" 
    script:
        "scripts/english/build_eng_tables.py"

# - LXX -
lxx_scripts = Path("scripts/lxx")
rule lxx_tables:
    input:
        sample=rules.get_samples.output.file, 
        lxx="_private_/verb_data/bhsa2lxx.json",
        scripts=list(lxx_scripts.glob("*.py")),
    output:
        lxx="../results/datasets/{verb}/lxx.csv"
    script:
        "scripts/lxx/build_lxx_tables.py"

# - apply some correction filters to the tables - 
# ! DEPRECATED ! as of yiqtol
#rule correct_verb:
#    input:
#        script="scripts/correct_{verb}.py",
#        files=expand('..results/datasets/{{verb}}/_{files}_.csv', files=results_csv),
#    output:
#        files=expand('..results/datasets/{{verb}}/{files}.csv', files=results_csv),
#    script:
#        "scripts/correct_{wildcards.verb}.py"

# - run correlation analyses for verb - 
results_csv = ['bhsa', 'bhsa_clrela', 'eng', 'eng_text', 'lxx']
rule analyze_verb:
    input:
        data_dir="../results/datasets/{verb}",
        data=expand("../results/datasets/{{verb}}/{results_csv}.csv", results_csv=results_csv),
        analyzers="scripts/analysis/analysis.py",
        script="scripts/analysis/{verb}/{analysis_name}.py"
    output:
        dir=directory("../results/analysis/{verb}/csv/{analysis_name}")
    script:
        "scripts/analysis/{wildcards.verb}/{wildcards.analysis_name}.py"

# - visualize analyses -
rule table_styles:
    input:
        "html/tables.css"
    output:
        dir=directory("../results/analysis/css"),
        css="../results/analysis/css/tables.css"
    shell:
        "cp {input} {output.dir}/."

visual_script = "scripts/analysis/visualize_results.py" 
rule visualize_verb:
    input:
        results="../results/analysis/{verb}/csv/{analysis_name}",
        visualizers="scripts/analysis/analysis_vis.py",
        script=visual_script,
        tablestyles=rules.table_styles.output.css
    output:
        dir=directory("../results/analysis/{verb}/html/{analysis_name}")
    script:
        visual_script

# construct a menu of html links to all of the respective analyses
verb = 'yqtl' # temporary work-around till I figure out a better way
analyses=[p.stem for p in Path(f'scripts/analysis/{verb}/').glob('*.py')]
rule html_menu:
    input: 
        analyses=expand('../results/analysis/{{verb}}/html/{analysis_name}', analysis_name=analyses),
        script="scripts/analysis/build_html_menu.py",
        results="../results/analysis/{verb}/html"
    output:
        html="../results/analysis/{verb}/html/menu.html"    
    script:
        "scripts/analysis/build_html_menu.py"
