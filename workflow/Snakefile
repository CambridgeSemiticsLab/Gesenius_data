# Snakemake Workflow for Gesenius Data 
# see https://snakemake.readthedocs.io

from pathlib import Path

configfile: "../config/config.yaml"
tf_modules = [
    # order strategically to set priorities for features
    "/Users/cody/github/CambridgeSemiticsLab/time_collocations/data/bhsa/tf", 
    "/Users/cody/github/etcbc/bhsa/tf/c", 
    "/Users/cody/github/etcbc/genre_synvar/tf/c", 
    "/Users/cody/github/etcbc/valence/tf/c",
    "/Users/cody/github/etcbc/heads/tf/c",
]

# -- clone necessary data -- 

# Need to complete the gitclone rule begun below
# see: https://github.com/snakemake/snakemake/issues/350
# rule get_bhsa:
#     output:
#         directory(expand("~/github/{package}", package=bhsa_packages))
#     shell:
#         """
#         if [[! -d "{wildcards.package}" ]]; then
#             git clone https://github.com/{}    
#         """

# -- make various mappings with GBI data --
rule preprocess_gbi:
    input:
        niv="_private_/GBI_alignment/niv84.ot.alignment.json",
        esv="_private_/GBI_alignment/esv.ot.alignment.json",    
    output:
        word_data="_private_/GBI_alignment/word_data.pickle",
        verse2words="_private_/GBI_alignment/verse2words.pickle",
        linkbyid="_private_/GBI_alignment/linkbyid.pickle",
        id2link="_private_/GBI_alignment/id2link.pickle",
    script: 
        "scripts/english/parse/preprocess_gbi.py"

# -- match 1 BHSA verbnode to 1 GBI word id
rule align_gbi_verbs:
    input:
        script="scripts/english/parse/align_verbs.py",
        bhsa2gbi="_private_/GBI_alignment/bhsa2gbi.json",
        tf_mods=tf_modules,
        word_data=rules.preprocess_gbi.output.word_data
    output:
        matches="_private_/GBI_alignment/verb_bhsa2gbi.json",
        no_match="_private_/GBI_alignment/verb_no_match.txt",
    script:
        "scripts/english/parse/align_verbs.py"

## -- parse english data --
#rule raw_parse_english:
#    input:
#        niv="_private_/GBI_alignment/niv84.ot.alignment.json",
#        esv="_private_/GBI_alignment/esv.ot.alignment.json", 
#        bhsa_links="_private/GBI_alignment/bhsa2gbi.json"
#    output:
#        esv="_private_/english_parsings/bhsa2esv.json",
#        niv="_private_/english_parsings/verb_data/bhsa2niv.json",
#        txt="_private_/english_parsings/GBI_alignment/verse2text.json",
#    script:
#        "scripts/english/parse/rawparse_english.py"

# -- generate samples --

rule get_samples:
    input:
        "/Users/cody/github/etcbc/bhsa/tf/c"
    output:
        directory("../results/samples"),
        expand("../results/samples/{verb}.json", verb=config["verb_forms"])
    script:
        "scripts/bhsa/get_samples.py"

# -- build csv tables --

# - BHSA -
bhsa_scripts = Path("scripts/bhsa")
main_bhsa="scripts/bhsa/build_bhsa_tables.py"
rule bhsa_tables:
    input:
        sample="../results/samples/{verb}.json",
        tf_mods=tf_modules,
        scripts=list(bhsa_scripts.glob('*.py'))
    output:
        bhsa=temp("../results/csv/{verb}/_bhsa_.csv"),
        bhsa_clrela=temp("../results/csv/{verb}/_bhsa_clrela_.csv"),
    script:
        main_bhsa

# - English -
eng_script="scripts/english/build_eng_tables.py"
rule eng_tables:
    input:
        sample="../results/samples/{verb}.json", 
        esv="_private_/verb_data/bhsa2esv.json",
        niv="_private_/verb_data/bhsa2niv.json",
        txt="_private_/GBI_alignment/verse2text.json",
        script=eng_script
    output: 
        eng=temp("../results/csv/{verb}/_eng_.csv"),
        eng_text=temp("../results/csv/{verb}/_eng_text_.csv") 
    script:
        eng_script

# - LXX -
lxx_script = "scripts/lxx/build_lxx_tables.py"
rule lxx_tables:
    input:
        sample="../results/samples/{verb}.json",
        lxx="_private_/verb_data/bhsa2lxx.json",
        script=lxx_script
    output:
        lxx=temp("../results/csv/{verb}/_lxx_.csv"),
    script:
        lxx_script

# - apply some correction filters to the tables - 
results_csv = ['bhsa', 'bhsa_clrela', 'eng', 'eng_text', 'lxx']
rule correct_verb:
    input:
        script="scripts/correct_{verb}.py",
        files=expand('../results/csv/{{verb}}/_{files}_.csv', files=results_csv),
    output:
        files=expand('../results/csv/{{verb}}/{files}.csv', files=results_csv),
    script:
        "scripts/correct_{wildcards.verb}.py"

# - run correlation analyses for verb - 
rule analyze_verb:
    input:
        data_dir="../results/csv/{verb}",
        data=expand("../results/csv/{{verb}}/{results_csv}.csv", results_csv=results_csv),
        analyzers="scripts/analysis/analysis.py",
        script="scripts/analysis/{verb}/{analysis_name}.py"
    output:
        dir=directory("../results/{verb}/csv/{analysis_name}")
    script:
        "scripts/analysis/{wildcards.verb}/{wildcards.analysis_name}.py"

# - visualize analyses -

rule table_styles:
    input:
        "html/tables.css"
    output:
        dir=directory("../results/css"),
        css="../results/css/tables.css"
    shell:
        "cp {input} {output.dir}/."

visual_script = "scripts/analysis/visualize_results.py" 
rule visualize_verb:
    input:
        results="../results/{verb}/csv/{analysis_name}",
        visualizers="scripts/analysis/analysis_vis.py",
        script=visual_script,
        tablestyles=rules.table_styles.output.css
    output:
        dir=directory("../results/{verb}/html/{analysis_name}")
    script:
        visual_script

# construct a menu of html links to all of the respective analyses
analyses = ['context', 'clause', 'verb_sem', 'translations', 'verb_lex']
rule html_menu:
    input: 
        analyses=expand("../results/{{verb}}/html/{analyses}", analyses=analyses),
        script="scripts/analysis/build_html_menu.py",
        results="../results/{verb}/html"
    output:
        html="../results/{verb}/html/menu.html"    
    script:
        "scripts/analysis/build_html_menu.py"
