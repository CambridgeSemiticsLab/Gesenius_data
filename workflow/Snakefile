# Snakemake Workflow for Gesenius Data 
# see https://snakemake.readthedocs.io

from pathlib import Path

tf_modules = [
    # order strategically to set priorities for features
    "/Users/cody/github/CambridgeSemiticsLab/time_collocations/data/bhsa/tf", 
    "/Users/cody/github/etcbc/bhsa/tf/c", 
    "/Users/cody/github/etcbc/genre_synvar/tf/c", 
    "/Users/cody/github/etcbc/valence/tf/c",
    "/Users/cody/github/etcbc/heads/tf/c",
]

# -- clone necessary data -- 

# Need to complete the gitclone rule begun below
# see: https://github.com/snakemake/snakemake/issues/350
# rule get_bhsa:
#     output:
#         directory(expand("~/github/{package}", package=bhsa_packages))
#     shell:
#         """
#         if [[! -d "{wildcards.package}" ]]; then
#             git clone https://github.com/{}    
#         """

# -- preprocess the raw GBI data --
rule preprocess_gbi:
    input:
        niv="_private_/GBI_alignment/niv84.ot.alignment.json",
        esv="_private_/GBI_alignment/esv.ot.alignment.json",    
    output:
        word_data="_private_/GBI_alignment/word_data.pickle",
        verse2words="_private_/GBI_alignment/verse2words.pickle",
        linkbyid="_private_/GBI_alignment/linkbyid.pickle",
        id2link="_private_/GBI_alignment/id2link.pickle",
    script: 
        "scripts/english/parse/preprocess_gbi.py"

# -- prepare GBI verb data for Spacy parsing --
# first match 1 BHSA verb node to 1 GBI word id
# then match the GBI word data to translation data
rule prepare_gbi_verbs:
    input:
        script="scripts/english/parse/prepare_verbs.py",
        bhsa2gbi="_private_/GBI_alignment/bhsa2gbi.json",
        tf_mods=tf_modules,
        word_data=rules.preprocess_gbi.output.word_data,
        id2link=rules.preprocess_gbi.output.id2link,
    output:
        bhsa2gbi="_private_/GBI_alignment/verb_bhsa2gbi.pickle",
        no_match="_private_/GBI_alignment/verb_no_match.txt",
        eng_verbs="_private_/GBI_alignment/english_verbs.pickle",
        versestoparse="_private_/GBI_alignment/versestoparse.pickle",
    script:
        "scripts/english/parse/prepare_verbs.py"

# -- automatically parse english data --
rule parse_eng_verses:
    input:
        versestoparse=rules.prepare_gbi_verbs.output.versestoparse,
        verse2words=rules.preprocess_gbi.output.verse2words,
        word_data=rules.preprocess_gbi.output.word_data,
        script="scripts/english/parse/parse_verses.py",
    output:
        parsedverses="_private_/GBI_alignment/parsings/parsedverses.pickle",
        verse2text="_private_/GBI_alignment/verse2text.json",
    script:
        "scripts/english/parse/parse_verses.py"

# -- apply automatic matches to parsed english data to get tense tags
rule parse_eng_tenses:
    input:
        main_script="scripts/english/parse/parse_tenses.py",
        impv_script="scripts/english/parse/imperatives.py",
        tense_rules="scripts/english/parse/tense_rules.py",
        parsedverses=rules.parse_eng_verses.output.parsedverses,
        eng_verbs=rules.prepare_gbi_verbs.output.eng_verbs,
        word_data=rules.preprocess_gbi.output.word_data,
        verse2words=rules.preprocess_gbi.output.verse2words,
    output:
        esv="_private_/GBI_alignment/parsings/bhsa2esv.json",
        niv="_private_/GBI_alignment/parsings/bhsa2niv.json",
        inspect_esv="_private_/GBI_alignment/parsings/inspect_esv_tenses.txt",
        inspect_niv="_private_/GBI_alignment/parsings/inspect_niv_tenses.txt",
    script:
        "scripts/english/parse/parse_tenses.py"

# -- generate Hebrew samples --
rule get_samples:
    input:
        tf_mod="/Users/cody/github/etcbc/bhsa/tf/c",
        bhsa2gbi=rules.prepare_gbi_verbs.output.bhsa2gbi,
        verbform_script="scripts/hebrew/verb_form.py",
        script="scripts/hebrew/get_samples.py",
    output:
        file="../results/samples/{verb}.json" 
    script:
        "scripts/hebrew/get_samples.py"

# -- apply manual corrections --
# How to work in manual corrections to the pipeline?
# We do it in a round-about way by keeping some outputs
# secret from snakemake and by designing dummy outputs.

esv_corr = "../results/datasets/esv_corrections.json"
niv_corr = "../results/datasets/niv_corrections.json"

# produce initial to-do files
rule get_eng_corrections:
    input:
        script="scripts/english/parse/get_corrections.py",
        script2="scripts/english/parse/correction_files.py",
        verse2text=rules.parse_eng_verses.output.verse2text,
        esv=rules.parse_eng_tenses.output.esv,
        niv=rules.parse_eng_tenses.output.niv,
        sample=rules.get_samples.output.file, 
    params:
        esv_corr=esv_corr,
        niv_corr=niv_corr,
    output:
        esv_todo="_private_/GBI_alignment/parsings/corrections/{verb}/esv.todo",
        niv_todo="_private_/GBI_alignment/parsings/corrections/{verb}/niv.todo",
    script:
        "scripts/english/parse/get_corrections.py"

# every time to-do file is changed, this rule will run;
# a dummy file "run_{verb}" is used as the target, but 
# the true output is another copy of the corrections and 
# to-do files, which have been updated to reflect the latest
# state of the to-do files.
# This is necessary to prevent the manual corrections file from being
# removed each time pipeline is run, which would destroy the manual work
rule apply_eng_corrections:
    input:
        script="scripts/english/parse/apply_corrections.py",
        script2="scripts/english/parse/correction_files.py",
        verse2text=rules.parse_eng_verses.output.verse2text,
        esv=rules.parse_eng_tenses.output.esv,
        niv=rules.parse_eng_tenses.output.niv,
        esv_todo=rules.get_eng_corrections.output.esv_todo, # changes to to-dos trigger run
        niv_todo=rules.get_eng_corrections.output.niv_todo,
    params:
        niv_corr=niv_corr,
        esv_corr=esv_corr,
    output:
        run="_private_/GBI_alignment/parsings/corrections/{verb}/run" # blank dummy file to make pipeline run
    script:
        "scripts/english/parse/apply_corrections.py"

# -- build csv tables --

# - BHSA -
hebrew_scripts = Path("scripts/hebrew")
main_hebrew="scripts/hebrew/build_hebrew_tables.py"
rule heb_tables:
    input:
        sample=rules.get_samples.output.file, 
        tf_mods=tf_modules,
        bhsa2gbi=rules.prepare_gbi_verbs.output.bhsa2gbi,
        scripts=list(hebrew_scripts.glob('*.py'))
    output:
        hebrew="../results/datasets/{verb}/heb.csv",
        hebrew_clrela="../results/datasets/{verb}/heb_clrela.csv",
    script:
        main_hebrew

# - English -
eng_scripts=Path("scripts/english")
rule eng_tables:
    input:
        sample=rules.get_samples.output.file, 
        esv=rules.parse_eng_tenses.output.esv,
        niv=rules.parse_eng_tenses.output.niv,
        niv_corr=niv_corr,
        esv_corr=esv_corr,
        run_corr=rules.apply_eng_corrections.output.run,
        parsedverses=rules.parse_eng_verses.output.parsedverses,
        scripts=list(eng_scripts.glob('*.py')),
    output: 
        eng="../results/datasets/{verb}/eng.csv",
        eng_text="../results/datasets/{verb}/eng_text.csv" 
    script:
        "scripts/english/build_eng_tables.py"

# - LXX -
lxx_scripts = Path("scripts/lxx")
rule lxx_tables:
    input:
        sample=rules.get_samples.output.file, 
        lxx="_private_/verb_data/bhsa2lxx.json",
        lxx_verses="_private_/verb_data/lxx_verses.json",
        scripts=list(lxx_scripts.glob("*.py")),
    output:
        lxx="../results/datasets/{verb}/lxx.csv",
    script:
        "scripts/lxx/build_lxx_tables.py"

# - apply some correction filters to the tables - 
# ! DEPRECATED ! as of yiqtol
#rule correct_verb:
#    input:
#        script="scripts/correct_{verb}.py",
#        files=expand('..results/datasets/{{verb}}/_{files}_.csv', files=results_csv),
#    output:
#        files=expand('..results/datasets/{{verb}}/{files}.csv', files=results_csv),
#    script:
#        "scripts/correct_{wildcards.verb}.py"

# - run correlation analyses for verb - 
rule analyze_verb:
    input:
        data_dir=rules.heb_tables.output + rules.eng_tables.output + rules.lxx_tables.output,
        analyzers="scripts/analysis/analysis.py",
        script="scripts/analysis/verbs/{verb}/{analysis_name}.py"
    output:
        dir=directory("../results/analysis/{verb}/csv/{analysis_name}")
    script:
        "scripts/analysis/verbs/{wildcards.verb}/{wildcards.analysis_name}.py"

# - visualize analyses -
rule table_styles:
    input:
        tables="html/tables.css",
        menu="html/menu_style.css"
    output:
        tables="../results/analysis/css/tables.css",
        menu="../results/analysis/css/menu_style.css"
    shell:
        "cp {input.tables} {output.tables}; cp {input.menu} {output.menu}"

visual_script = "scripts/analysis/visualize_results.py" 
rule visualize_verb:
    input:
        results=rules.analyze_verb.output.dir,
        visualizers="scripts/analysis/analysis_vis.py",
        script=visual_script,
        tablestyles=rules.table_styles.output.tables,
        menustyle=rules.table_styles.output.menu
    output:
        dir=directory("../results/analysis/{verb}/html/{analysis_name}")
    script:
        visual_script

# construct a menu of html links to all of the respective analyses
def get_analyses(wildcards):
    """Formats analyses names automatically."""
    scripts_path = Path(f'scripts/analysis/verbs/{wildcards.verb}')
    analyses = [p.stem for p in scripts_path.glob('*.py')]
    return expand(f'../results/analysis/{wildcards.verb}/html/{{analysis_name}}', analysis_name=analyses)

rule html_menu:
    input: 
        analyses=get_analyses,
        script="scripts/analysis/build_html_menu.py",
    params:
        results="../results/analysis/{verb}/html",
    output:
        html="../results/analysis/{verb}/html/menu.html"    
    script:
        "scripts/analysis/build_html_menu.py"
