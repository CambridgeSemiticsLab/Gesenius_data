{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build omnibus corrections files\n",
    "\n",
    "The pipeline currently splits up corrections files into separate jsons. But there is actually no need for this.\n",
    "\n",
    "This is actually relevant now that I am going to be splitting up the yiqtols from cohortatives and jussives. \n",
    "Those extra verb forms are already corrected, but in the yiqtol file. But it shouldn't matter. All corrections\n",
    "should go into a single file, mapped by BHSA node. I will merge them all here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--BEFORE SIZES--\n",
      "esv yqtl: 1377\n",
      "niv yqtl: 1455\n",
      "esv wqtl: 1304\n",
      "niv wqtl: 1151\n",
      "total esv: 2681\n",
      "total niv: 2606\n",
      "\n",
      "--AFTER SIZES--\n",
      "total esv: 2681\n",
      "total niv: 2606\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "datasets_dir = Path('../../../results/datasets')\n",
    "corr_esv_yqtl = json.loads(datasets_dir.joinpath('yqtl/corr_esv_yqtl.json').read_text())\n",
    "corr_niv_yqtl = json.loads(datasets_dir.joinpath('yqtl/corr_niv_yqtl.json').read_text())\n",
    "corr_esv_wqtl = json.loads(datasets_dir.joinpath('wqtl/corr_esv_wqtl.json').read_text())\n",
    "corr_niv_wqtl = json.loads(datasets_dir.joinpath('wqtl/corr_niv_wqtl.json').read_text())\n",
    "\n",
    "print('--BEFORE SIZES--')\n",
    "print('esv yqtl:', len(corr_esv_yqtl))\n",
    "print('niv yqtl:', len(corr_niv_yqtl))\n",
    "print('esv wqtl:', len(corr_esv_wqtl))\n",
    "print('niv wqtl:', len(corr_niv_wqtl))\n",
    "print('total esv:', len(corr_esv_yqtl)+len(corr_esv_wqtl))\n",
    "print('total niv:', len(corr_niv_yqtl)+len(corr_niv_wqtl))\n",
    "print()\n",
    "\n",
    "corrs = {\n",
    "    'esv': [corr_esv_yqtl, corr_esv_wqtl],\n",
    "    'niv': [corr_niv_yqtl, corr_niv_wqtl],\n",
    "}\n",
    "\n",
    "corr_file = collections.defaultdict(dict)\n",
    "\n",
    "for trans, datasets in corrs.items():\n",
    "    for dataset in datasets:\n",
    "        corr_file[trans].update(dataset)\n",
    "        \n",
    "print('--AFTER SIZES--')\n",
    "for trans, data in corr_file.items():\n",
    "    print(f'total {trans}:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "for trans, data in corr_file.items():\n",
    "    outpath = datasets_dir.joinpath(f'{trans}_corrections.json')\n",
    "    with open(outpath, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
