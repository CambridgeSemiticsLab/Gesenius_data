{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling GBI Alignments into Workflow\n",
    "\n",
    "GBI has graciously provided alignments for the ESV, KJV, and NIV.\n",
    "Unfortunately those alignments are closed-source, so this notebook\n",
    "will only provide code used for interpreting the aligned JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b title=\"local commit\">TF-app:</b> <span title=\"#113c0687cfce3077734dac1844d244d20f4ace6f offline under ~/text-fabric-data\">~/text-fabric-data/annotation/app-bhsa/code</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv1.6 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/bhsa/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"r1.2 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/phono/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"r1.2 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/parallels/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 8.4.0</a>, <a target=\"_blank\" href=\"https://github.com/annotation/app-bhsa\" title=\"bhsa TF-app\">app-bhsa</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br><b>Data:</b> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/0_home\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/writing/hebrew.html\" title=\"How TF features represent text\">Character table</a>, <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/0_home\" title=\"BHSA feature documentation\">Feature docs</a><br><b>Features:</b><br><details><summary><b>Parallel Passages</b></summary><b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"~/text-fabric-data/etcbc/parallels/tf/c/crossref.tf\">crossref</a></i></b><br></details><details><summary><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b></summary><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/book.tf\">book</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book@ll\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/book@am.tf\">book@ll</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/chapter\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/chapter.tf\">chapter</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/code\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/code.tf\">code</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/det\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/det.tf\">det</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/domain\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/domain.tf\">domain</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/freq_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/freq_lex.tf\">freq_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/function\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/function.tf\">function</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_cons.tf\">g_cons</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_cons_utf8.tf\">g_cons_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_lex.tf\">g_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_lex_utf8.tf\">g_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_word.tf\">g_word</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_word_utf8.tf\">g_word_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gloss\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/gloss.tf\">gloss</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/gn.tf\">gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/label\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/label.tf\">label</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/language\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/language.tf\">language</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/lex.tf\">lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/lex_utf8.tf\">lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ls\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/ls.tf\">ls</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nametype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nametype.tf\">nametype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nme\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nme.tf\">nme</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nu.tf\">nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/number\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/number.tf\">number</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/otype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pargr\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pargr.tf\">pargr</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pdp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pdp.tf\">pdp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pfm\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pfm.tf\">pfm</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs.tf\">prs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_gn.tf\">prs_gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_nu.tf\">prs_nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_ps.tf\">prs_ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/ps.tf\">ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere.tf\">qere</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer.tf\">qere_trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer_utf8.tf\">qere_trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_utf8.tf\">qere_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rank_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/rank_lex.tf\">rank_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rela\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/rela.tf\">rela</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/sp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/sp.tf\">sp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/st\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/st.tf\">st</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/tab\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/tab.tf\">tab</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/trailer.tf\">trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/trailer_utf8.tf\">trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/txt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/txt.tf\">txt</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/typ\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/typ.tf\">typ</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/uvf\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/uvf.tf\">uvf</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbe\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vbe.tf\">vbe</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vbs.tf\">vbs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/verse\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/verse.tf\">verse</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/voc_lex.tf\">voc_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/voc_lex_utf8.tf\">voc_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vs.tf\">vs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vt.tf\">vt</a><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/mother\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/mother.tf\">mother</a></i></b><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/oslots\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/oslots.tf\">oslots</a></i></b><br></details><details><summary><b>Phonetic Transcriptions</b></summary><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/c/phono.tf\">phono</a><br><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/c/phono_trailer.tf\">phono_trailer</a><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/server/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 0.1rem;\n",
       "    margin: 0.1rem;\n",
       "    direction: ltr;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1rem 0rem;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".section {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--section);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  0.5rem 0.1rem 0.1rem 0.1rem;\n",
       "    margin: 0.8rem 0.1rem 0.1rem 0.1rem;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 0.2rem;\n",
       "    margin-left: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 0.2rem;\n",
       "    margin-right: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -1.2rem;\n",
       "    margin-left: 1rem;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3rem;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 0.1rem;\n",
       "    margin-left: 0.1rem;\n",
       "    padding: 0.1rem 0.1rem;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 2rem;\n",
       "\tpadding: 1rem;\n",
       "\tborder: 0.1rem solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--section:            hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         0.15rem;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   0.1rem;\n",
       "  --border-width0:      0.1rem;\n",
       "  --border-width1:      0.15rem;\n",
       "  --border-width2:      0.2rem;\n",
       "  --border-width3:      0.3rem;\n",
       "  --border-width4:      0.25rem;\n",
       "  --border-width-plain: 0.1rem;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 0.2rem ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 0.2rem ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.3rem;\n",
       "  padding: 0.2rem;\n",
       "  margin: 0.2rem;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import unicodedata as unicode\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import Levenshtein\n",
    "\n",
    "# import BHSA data with TF to look at alignment possibilities\n",
    "from tf.app import use\n",
    "\n",
    "# organize pathways\n",
    "PROJ_DIR = Path.home().joinpath('github/CambridgeSemiticsLab/translation_traditions_HB')\n",
    "GBI_DATA_DIR = PROJ_DIR.joinpath('data/_private_/GBI_alignment')\n",
    "\n",
    "# load BHSA data\n",
    "bhsa = use('bhsa')\n",
    "api = bhsa.api\n",
    "F, E, T, L, Fs, = api.F, api.E, api.T, api.L, api.Fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore GBI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['niv84.ot.alignment', 'kjv.ot.alignment', 'esv.ot.alignment'])\n"
     ]
    }
   ],
   "source": [
    "file2data = {}\n",
    "for file in GBI_DATA_DIR.glob('*.json'):\n",
    "    if 'ot' in file.name:\n",
    "        file2data[file.stem] = json.loads(file.read_text())\n",
    "        \n",
    "print('keys:', file2data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23202"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check out the NIV OT alignment\n",
    "\n",
    "niv_data = file2data['niv84.ot.alignment']\n",
    "\n",
    "len(niv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['manuscript', 'translation', 'links'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_verse = niv_data[1]\n",
    "\n",
    "# parts of individual entry\n",
    "ex_verse.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex_verse['links'] # look at the alignment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "פְּנֵ֣י -> surface of\n",
      "ר֣וּחַ -> Spirit of\n",
      "מְרַחֶ֖פֶת -> was hovering\n"
     ]
    }
   ],
   "source": [
    "# experiment with multiple word link: \"was hovering\"\n",
    "\n",
    "def get_trans_text(manu, trans, verse):\n",
    "    \"\"\"Join text/translated words for comparison\"\"\"\n",
    "    heb_txt = ' '.join(verse['manuscript']['words'][h]['text'] for h in manu).strip('\\u200e')\n",
    "    eng_txt = ' '.join(verse['translation']['words'][e]['text'] for e in sorted(trans))\n",
    "    return (heb_txt, eng_txt)\n",
    "    \n",
    "for manu, trans in ex_verse['links']:\n",
    "    if len(trans) > 1:\n",
    "        trans = sorted(trans)\n",
    "        heb_txt, eng_txt = get_trans_text(manu, trans, ex_verse)\n",
    "        print(f'{heb_txt} -> {eng_txt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('בָּרָ֣א', 'created'),\n",
       " ('הָיְתָ֥ה', 'was'),\n",
       " ('מְרַחֶ֖פֶת', 'was hovering'),\n",
       " ('יֹּ֥אמֶר', 'said'),\n",
       " ('יְהִ֣י', 'Let there be')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_dataset = []\n",
    "\n",
    "# experiment with collecting verbs in HB\n",
    "for verse in niv_data:\n",
    "    for manu, trans in verse['links']:\n",
    "        _mainword_ = manu[0]\n",
    "        if verse['manuscript']['words'][_mainword_]['pos'] == 'verb':\n",
    "            heb_txt, eng_txt = get_trans_text(manu, trans, verse)\n",
    "            verb_dataset.append((heb_txt, eng_txt))\n",
    "            \n",
    "verb_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('בָּרָ֣א', 'created'),\n",
       " ('הָיְתָ֥ה', 'was'),\n",
       " ('מְרַחֶ֖פֶת', 'was hovering'),\n",
       " ('יֹּ֥אמֶר', 'said'),\n",
       " ('יְהִ֣י', 'Let there be'),\n",
       " ('יְהִי־', 'there was'),\n",
       " ('יַּ֧רְא', 'saw'),\n",
       " ('יַּבְדֵּ֣ל', 'separated'),\n",
       " ('יִּקְרָ֨א', 'called'),\n",
       " ('קָ֣רָא', 'he called')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex_verse['translation']['words'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex_verse['manuscript']['words'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_verse['manuscript']['words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment with BHSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will seek to align the Hebrew texts on the basis of the consonantal text. For this, we take\n",
    "some cues from Dirk Roorda's alignment efforts between BHSA and OSM: \n",
    "https://github.com/ETCBC/bridging/blob/master/programs/BHSAbridgeOSM.ipynb\n",
    "\n",
    "The essential issue that makes alignment difficult is word divisions, which differ between various \n",
    "texts. There is an added difficulty with the GBI Hebrew data, which is based on Westminster Leningrad\n",
    "Codex (WLC), since that texts sometimes goes with the ketiv or other times with qere.\n",
    "\n",
    "## Alignment Strategy\n",
    "\n",
    "One option to align the texts would be to iterate word-by-word while keeping track of the current\n",
    "position in the text. If BHSA were the reference point, we'd iterate over all word nodes in BHSA\n",
    "and attempt to match with the next WLC word in the set; this would require us to advance the position\n",
    "until a match is composed in cases where BHSA word is longer than WLC, while vice versa when BHSA is \n",
    "shorter than WLC. Dirk Roorda has followed this strategy in the BHSA // OSM alignment.\n",
    "\n",
    "Another option, which we shall follow here, is to use verse identity to map positions in \n",
    "both BHSA and WLC to a common reference string. For example, consider the following string:\n",
    "\n",
    "> \"A cat jumped up\"\n",
    "\n",
    "Let there be textA and textB, which each index this string differently as follows:\n",
    "\n",
    "```\n",
    "            0        1        2\n",
    "textA = [\"A cat\", \"jumped\", \"up\"]\n",
    "\n",
    "          0     1        2       3\n",
    "textB = [\"A\", \"cat\", \"jumped\", \"up\"]\n",
    "```\n",
    "\n",
    "The underlying string for both texts is the same however, and can be joined in such a \n",
    "way as to produce an identical string with its own indices: \n",
    "\n",
    "```\n",
    "          0 1 2 3 4 5 6 7 8 9 10 11\n",
    "string = \"A c a t j u m p e d  u  p\"\n",
    "          \n",
    "```\n",
    "\n",
    "We can use this identity property as a common reference point to which the text positions can\n",
    "be mapped and translated:\n",
    "\n",
    "```\n",
    " [ 0 ] -> [0, 1, 2, 3] <- [0, 1] \n",
    "textA        string        textB\n",
    "\n",
    "[1] -> [4, 5, 6, 7, 8, 9] <- [2]\n",
    "textA        string         textB\n",
    "\n",
    "[2] -> [10, 11] <- [3]\n",
    "textA   string     textB\n",
    "```\n",
    "\n",
    "Using these mappings, we can produce an alignment:\n",
    "\n",
    "```\n",
    "[\n",
    "    [[0], [0, 1]],\n",
    "    [[1], [2]],\n",
    "    [[2], [3]]\n",
    "]\n",
    "```\n",
    "\n",
    "Since verses are identical between the two sources, we need only ensure that verse strings\n",
    "are likewise identical wherever possible. This is done by converting the text in both\n",
    "sources to its consonantal form, stripping it of characters unique to each source, stripping\n",
    "spaces, and joining the text on nothing to create an indexable string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess verse strings\n",
    "\n",
    "The first step here is to build the verse strings which can be used for indexing. \n",
    "This is also necessary to demonstrate that our strategy will work, and also to \n",
    "catch those exceptional cases where it won't.\n",
    "\n",
    "We need to do a few things in this preprocessing stage:\n",
    "\n",
    "    1. recognize verse id tags from GBI dataset and convert to TF reference tuples\n",
    "    2. GBI alignments are organized into lists, which mostly correspond with verses,\n",
    "        but not always. To aid the alignment work, we should fix this discrepancy by\n",
    "        following the Hebrew versification and utilizing word id links rather than \n",
    "        indices.\n",
    "    3. prepare strings and indices for alignment\n",
    "   \n",
    "    \n",
    "### Exploratory analysis of word ids\n",
    "\n",
    "We look at the length of GBI word ids to write a regex pattern that can\n",
    "recognize each of the parts. The word ids contain versification information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 295168), (11, 179844)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference data stored at word level in 'id' key\n",
    "# I expect 'id' length to vary by 1, thus 11 or 12\n",
    "\n",
    "id_lengths = collections.Counter()\n",
    "\n",
    "for verse in niv_data:\n",
    "    for word in verse['manuscript']['words']:\n",
    "        id_lengths[len(str(word['id']))] += 1\n",
    "        \n",
    "id_lengths.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some book ID integers have 11 for single-digit books. We'll correct for this in `id2etcbc` by \n",
    "adding a 0 padding to normalize this difference for the regex matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Function for converting ids to TF reference tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBI uses English book order, so we need to make an int 2 book mapping using that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng book order\n",
    "eng_book_list = '''\n",
    "Genesis\n",
    "Exodus\n",
    "Leviticus\n",
    "Numbers\n",
    "Deuteronomy\n",
    "Joshua\n",
    "Judges\n",
    "Ruth\n",
    "1 Samuel\n",
    "2 Samuel\n",
    "1 Kings\n",
    "2 Kings\n",
    "1 Chronicles\n",
    "2 Chronicles\n",
    "Ezra\n",
    "Nehemiah\n",
    "Esther\n",
    "Job\n",
    "Psalms\n",
    "Proverbs\n",
    "Ecclesiastes\n",
    "Song of songs\n",
    "Isaiah\n",
    "Jeremiah\n",
    "Lamentations\n",
    "Ezekiel\n",
    "Daniel\n",
    "Hosea\n",
    "Joel\n",
    "Amos\n",
    "Obadiah\n",
    "Jonah\n",
    "Micah\n",
    "Nahum\n",
    "Habakkuk\n",
    "Zephaniah\n",
    "Haggai\n",
    "Zechariah\n",
    "Malachi\n",
    "'''.strip().replace(' ', '_').split()\n",
    "\n",
    "# map to integers\n",
    "int2book = {\n",
    "    i+1: book for i, book in enumerate(eng_book_list)\n",
    "}\n",
    "\n",
    "# regex pattern for matching word ID info to its parts\n",
    "# e.g. ('01', '001', '001', '001', '1')\n",
    "# i.e. (bookN, chapterN, verseN, wordN, partN)\n",
    "ref_id_re = re.compile('([0-9]{2})([0-9]{3})([0-9]{3})([0-9]{3})([1-9])')\n",
    "\n",
    "def id2ref(id_int):\n",
    "    \"\"\"Convert GBI ID ref tag to TF ref tuple\"\"\"\n",
    "    \n",
    "    id_str = str(id_int)\n",
    "    \n",
    "    # fix ambiguity with lack of book padding in single-digit books\n",
    "    if len(id_str) == 11:\n",
    "        id_str = '0' + id_str\n",
    "        \n",
    "    # match parts\n",
    "    bookN, chapterN, verseN, wordN, partN = ref_id_re.match(id_str).groups()\n",
    "    \n",
    "    book = int2book.get(int(bookN))\n",
    "    chapter = int(chapterN)\n",
    "    verse = int(verseN)\n",
    "    \n",
    "    return (book, chapter, verse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a sanity check for book order. I'll do this by looking at the \n",
    "verse counts for each book. If we've got the book order wrong, we'll see some\n",
    "anomalous counts.\n",
    "\n",
    "[postscript: there was an anomaly when I first tried with Heb. book order, with\n",
    "Jonah showing a verse count of 2,523 😂, now it is fixed with Eng order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "verse_data = []\n",
    "\n",
    "# compile a dataset for easy dataframe statistics\n",
    "for verse in niv_data:\n",
    "    first_word = verse['manuscript']['words'][0]\n",
    "    book, chapter, verse = id2ref(first_word['id'])\n",
    "    verse_data.append({'book': book, 'chapter': chapter, 'verse': verse})\n",
    "    \n",
    "verse_data_df = pd.DataFrame(verse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>verse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      book  chapter  verse\n",
       "0  Genesis        1      1\n",
       "1  Genesis        1      2\n",
       "2  Genesis        1      3\n",
       "3  Genesis        1      4\n",
       "4  Genesis        1      5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Psalms           2523\n",
       "Genesis          1533\n",
       "Jeremiah         1364\n",
       "Isaiah           1291\n",
       "Numbers          1288\n",
       "Ezekiel          1273\n",
       "Exodus           1213\n",
       "Job              1070\n",
       "Deuteronomy       959\n",
       "1_Chronicles      942\n",
       "Proverbs          915\n",
       "Leviticus         859\n",
       "2_Chronicles      822\n",
       "1_Kings           813\n",
       "1_Samuel          810\n",
       "2_Kings           719\n",
       "2_Samuel          695\n",
       "Joshua            658\n",
       "Judges            618\n",
       "Nehemiah          405\n",
       "Daniel            357\n",
       "Ezra              280\n",
       "Ecclesiastes      222\n",
       "Zechariah         211\n",
       "Hosea             197\n",
       "Esther            167\n",
       "Lamentations      154\n",
       "Amos              146\n",
       "Song_of_songs     117\n",
       "Micah             105\n",
       "Ruth               85\n",
       "Joel               73\n",
       "Habakkuk           56\n",
       "Malachi            55\n",
       "Zephaniah          53\n",
       "Jonah              48\n",
       "Nahum              47\n",
       "Haggai             38\n",
       "Obadiah            21\n",
       "Name: book, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count n verses by book\n",
    "\n",
    "verse_data_df.book.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1070"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L.d(T.nodeFromSection(('Job',)), 'verse')) # sanity check Job verse length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess GBI data and convert to dict of word ids with links paired to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbi_words = {}\n",
    "\n",
    "for pseudo_verse in niv_data:\n",
    "    \n",
    "    # unpack hebrew, english, and alignment data\n",
    "    hebrew_words = pseudo_verse['manuscript']['words']\n",
    "    english_words = pseudo_verse['translation']['words']\n",
    "    links = pseudo_verse['links']\n",
    "    \n",
    "    # enter all hebrew words into gbi words\n",
    "    # words not included in the translation are not included in the links\n",
    "    # e.g. את in Gen 1:1 has no entry in links\n",
    "    # so we must initialize all words with an empty links item\n",
    "    for hw in hebrew_words:\n",
    "        hw_id = hw['id']\n",
    "        hw['tf_ref'] = id2ref(hw_id) # store TF tuple ref\n",
    "        hw['links'] = [] # compile linked english word data here\n",
    "        hw['trans_span'] = [] # see below\n",
    "        gbi_words[hw_id] = hw\n",
    "\n",
    "    # add alignment to words that have it\n",
    "    for link in links:\n",
    "    \n",
    "        heb_indices, eng_indices = link\n",
    "            \n",
    "        # we need to keep track of those cases where\n",
    "        # multiple Hebrew words are covered by the same\n",
    "        # english translation string; these are cases where\n",
    "        # the Hebrew side of the link contains more than 1 element\n",
    "        # thus we build a list of all ids in the Heb side that will be \n",
    "        # added as a key of the word's dictionary, trans_span\n",
    "        heb_ids = [hebrew_words[i]['id'] for i in heb_indices]\n",
    "        \n",
    "        # store links and verse id data under word dict\n",
    "        # and prepare word dict to be stored in gbi_words\n",
    "        for hi in heb_indices:\n",
    "            hword_id = hebrew_words[hi]['id']\n",
    "            hword = gbi_words[hword_id]\n",
    "            hword['trans_span'] = sorted(heb_ids) # list of peer Hebrew words in translation\n",
    "            \n",
    "            # collect linked english word data\n",
    "            for ei in eng_indices:\n",
    "                eword = english_words[ei]\n",
    "                hword['links'].append(eword)\n",
    "                \n",
    "            # sort the english links \n",
    "            # preprocess english gloss tag\n",
    "            # put it in the dict\n",
    "            hword['links'] = sorted(hword['links'], key=lambda k: k['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print example with multiple links in translation\n",
    "# for w, wdat in gbi_words.items():\n",
    "#     if len(wdat['links']) > 2:\n",
    "#         pprint(wdat)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbi_words[10010010041] # NB empty links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build normalized verse strings with maps to ids\n",
    "\n",
    "We will also do this for BHSA, with maps to word node number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define patterns and functions which are used to\n",
    "# normalize both Hebrew texts to a plain consonantal\n",
    "# version without punctuation or spacing\n",
    "\n",
    "# pattern matches only Heb consonants for filtering\n",
    "heb_cons = re.compile('[\\u05D0-\\u05EA]')\n",
    "\n",
    "# to normalize final letters\n",
    "final_letters = {\n",
    "    'ך':\\\n",
    "    'כ',\n",
    "    'ם':\\\n",
    "    'מ',\n",
    "    'ן':\\\n",
    "    'נ',\n",
    "    'ף':\\\n",
    "    'פ',\n",
    "    'ץ':\\\n",
    "    'צ',\n",
    "}\n",
    "\n",
    "def unFinal(s): \n",
    "    \"\"\"\"Replace final Heb letters with non-final version.\n",
    "    \n",
    "    Credit Dirk Roorda\n",
    "    \"\"\"\n",
    "    return ''.join(final_letters.get(c, c) for c in s)\n",
    "\n",
    "def normalize_string(string):\n",
    "    \"\"\"Normalize BHSA/WLC strings to make them comparable.\"\"\"\n",
    "    string = unicode.normalize('NFD', string) # normalize chars\n",
    "    string = ''.join(heb_cons.findall(string)) # strip vowels/points/other chars\n",
    "    string = unFinal(string) # disambiguate final letters\n",
    "    string = string.replace('\\u200e', '') # remove RL character (GBI)\n",
    "    string = string.replace(' ', '') # remove any latent spaces\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BHSA, we need to make two verse strings: one for ketiv, one for qere. Thus,\n",
    "we will plan to have multiple strings and associated indices for each one for both\n",
    "bhsa and gbi verses. The algorithm will then pick out any matching pair and use that as\n",
    "the basis for the alignment. Allowing multiple strings for GBI/WLC allows for the \n",
    "possibility of solving exceptions with manually added strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_string_data(word_ids, string_instructs):\n",
    "    \"\"\"Build strings from words with index maps.\n",
    "    \n",
    "    Uses the data in string_instructs to convert each word\n",
    "    into a string and add that string to a large string. \n",
    "    Maps each word id to a span of character indices for the larger\n",
    "    string, telling which indices correspond with a given word.\n",
    "    \n",
    "    Args:\n",
    "        word_ids: list of word ids unique to BHSA or WLC\n",
    "        string_instructs: list of paired string names / string-making functions\n",
    "            where string-maker takes a word id and converts to string\n",
    "        \n",
    "    Returns:\n",
    "        list of three-tuples of name, string, and index mappings for words.\n",
    "    \"\"\"\n",
    "    indices = collections.defaultdict(list)\n",
    "    string_data = []\n",
    "    \n",
    "    # build data for each string type\n",
    "    for name, stringifier in string_instructs:\n",
    "        \n",
    "        string = ''\n",
    "        index = -1\n",
    "        mapping = collections.defaultdict(list)\n",
    "        \n",
    "        for word in word_ids:\n",
    "            for c in stringifier(word):\n",
    "                string += c\n",
    "                index += 1\n",
    "                mapping[word].append(index)\n",
    "                \n",
    "        string_data.append((name, string, mapping))\n",
    "    \n",
    "    return string_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- build strings and indices for both BHSA and WLC -- \n",
    "# store them in verse_strings for alignment in next step\n",
    "\n",
    "verse_strings = collections.defaultdict(dict)\n",
    "    \n",
    "# -- 1. BHSA --\n",
    "\n",
    "# define a bunch of stringifier functions\n",
    "# the variant strings produced by these\n",
    "# functions will be compared pairwise with the \n",
    "# gbi strings to look for any pairwise match; \n",
    "# having numerous variants allows for a more robust\n",
    "# matching process\n",
    "\n",
    "def bhsa_qere(word):\n",
    "    \"\"\"Generate qere strings from BHSA word\"\"\"\n",
    "    string = F.qere_utf8.v(word) or F.g_word_utf8.v(word)\n",
    "    string = normalize_string(string)\n",
    "    return string\n",
    "    \n",
    "def bhsa_ketiv(word):\n",
    "    \"\"\"Generate ketiv strings from BHSA word\"\"\"\n",
    "    string = F.g_word_utf8.v(word)\n",
    "    string = normalize_string(string)\n",
    "    return string\n",
    "\n",
    "def bhsa_qere_art(word):\n",
    "    \"\"\"Generate qere with article\"\"\"\n",
    "    string = bhsa_qere(word)\n",
    "    if not string and F.lex.v(word) == 'H':\n",
    "        string = 'ה'\n",
    "    return string\n",
    "    \n",
    "def bhsa_ketiv_art(word):\n",
    "    \"\"\"Generate ketiv with article\"\"\"\n",
    "    string = bhsa_ketiv(word)\n",
    "    if not string and F.lex.v(word) == 'H':\n",
    "        string = 'ה'\n",
    "    return string\n",
    "\n",
    "# iterate through all BHSA verses and build strings/indices\n",
    "for verse in F.otype.s('verse'):\n",
    "    ref_tuple = T.sectionFromNode(verse)\n",
    "    verse_words = L.d(verse, 'word')\n",
    "    string_instructs = [\n",
    "        ('qere', bhsa_qere),\n",
    "        ('ketiv', bhsa_ketiv),\n",
    "        ('qere+art', bhsa_qere_art),\n",
    "        ('ketiv+art', bhsa_ketiv_art),\n",
    "    ]\n",
    "    string_data = build_string_data(verse_words, string_instructs)\n",
    "    verse_strings['bhsa'][ref_tuple] = string_data\n",
    "    \n",
    "# -- 2. WLC --\n",
    "\n",
    "def gbi_string(word):\n",
    "    \"\"\"Generate string for GBI word\"\"\"\n",
    "    string = gbi_words[word]['text']\n",
    "    string = normalize_string(string)\n",
    "    return string\n",
    "\n",
    "def gbi_string_art(word):\n",
    "    \"\"\"Build string that has vocalized article\n",
    "    \n",
    "    Vocalization of articles can differ between\n",
    "    the two sources.\n",
    "    \"\"\"\n",
    "    string = gbi_string(word)\n",
    "    if not string and gbi_words[word]['lemma'] == 'הַ':\n",
    "        string = 'ה'\n",
    "    return string\n",
    "\n",
    "# cluster GBI/WLC words into verses\n",
    "gbi_verses = collections.defaultdict(list)\n",
    "for word, word_data in gbi_words.items():\n",
    "    gbi_verses[word_data['tf_ref']].append(word_data['id'])\n",
    "\n",
    "# iterate through verses and build strings/indices\n",
    "for ref_tuple, words in gbi_verses.items():\n",
    "    string_instructs = [\n",
    "        ('string', gbi_string),\n",
    "        ('string+art', gbi_string_art),\n",
    "    ]\n",
    "    string_data = build_string_data(words, string_instructs)\n",
    "    verse_strings['gbi'][ref_tuple] = string_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a sample of the mappings / strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('qere',\n",
       "  'בראשיתבראאלהימאתהשמימואתהארצ',\n",
       "  defaultdict(list,\n",
       "              {1: [0],\n",
       "               2: [1, 2, 3, 4, 5],\n",
       "               3: [6, 7, 8],\n",
       "               4: [9, 10, 11, 12, 13],\n",
       "               5: [14, 15],\n",
       "               6: [16],\n",
       "               7: [17, 18, 19, 20],\n",
       "               8: [21],\n",
       "               9: [22, 23],\n",
       "               10: [24],\n",
       "               11: [25, 26, 27]})),\n",
       " ('ketiv',\n",
       "  'בראשיתבראאלהימאתהשמימואתהארצ',\n",
       "  defaultdict(list,\n",
       "              {1: [0],\n",
       "               2: [1, 2, 3, 4, 5],\n",
       "               3: [6, 7, 8],\n",
       "               4: [9, 10, 11, 12, 13],\n",
       "               5: [14, 15],\n",
       "               6: [16],\n",
       "               7: [17, 18, 19, 20],\n",
       "               8: [21],\n",
       "               9: [22, 23],\n",
       "               10: [24],\n",
       "               11: [25, 26, 27]})),\n",
       " ('qere+art',\n",
       "  'בראשיתבראאלהימאתהשמימואתהארצ',\n",
       "  defaultdict(list,\n",
       "              {1: [0],\n",
       "               2: [1, 2, 3, 4, 5],\n",
       "               3: [6, 7, 8],\n",
       "               4: [9, 10, 11, 12, 13],\n",
       "               5: [14, 15],\n",
       "               6: [16],\n",
       "               7: [17, 18, 19, 20],\n",
       "               8: [21],\n",
       "               9: [22, 23],\n",
       "               10: [24],\n",
       "               11: [25, 26, 27]})),\n",
       " ('ketiv+art',\n",
       "  'בראשיתבראאלהימאתהשמימואתהארצ',\n",
       "  defaultdict(list,\n",
       "              {1: [0],\n",
       "               2: [1, 2, 3, 4, 5],\n",
       "               3: [6, 7, 8],\n",
       "               4: [9, 10, 11, 12, 13],\n",
       "               5: [14, 15],\n",
       "               6: [16],\n",
       "               7: [17, 18, 19, 20],\n",
       "               8: [21],\n",
       "               9: [22, 23],\n",
       "               10: [24],\n",
       "               11: [25, 26, 27]}))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_strings['bhsa'][('Genesis', 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('string',\n",
       "  'בראשיתבראאלהימאתהשמימואתהארצ',\n",
       "  defaultdict(list,\n",
       "              {10010010011: [0],\n",
       "               10010010012: [1, 2, 3, 4, 5],\n",
       "               10010010021: [6, 7, 8],\n",
       "               10010010031: [9, 10, 11, 12, 13],\n",
       "               10010010041: [14, 15],\n",
       "               10010010051: [16],\n",
       "               10010010052: [17, 18, 19, 20],\n",
       "               10010010061: [21],\n",
       "               10010010062: [22, 23],\n",
       "               10010010071: [24],\n",
       "               10010010072: [25, 26, 27]})),\n",
       " ('string+art',\n",
       "  'בראשיתבראאלהימאתהשמימואתהארצ',\n",
       "  defaultdict(list,\n",
       "              {10010010011: [0],\n",
       "               10010010012: [1, 2, 3, 4, 5],\n",
       "               10010010021: [6, 7, 8],\n",
       "               10010010031: [9, 10, 11, 12, 13],\n",
       "               10010010041: [14, 15],\n",
       "               10010010051: [16],\n",
       "               10010010052: [17, 18, 19, 20],\n",
       "               10010010061: [21],\n",
       "               10010010062: [22, 23],\n",
       "               10010010071: [24],\n",
       "               10010010072: [25, 26, 27]}))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_strings['gbi'][('Genesis', 1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of string match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse_strings['bhsa'][('Genesis', 1, 1)][0][1] == verse_strings['gbi'][('Genesis', 1, 1)][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Algorithm\n",
    "\n",
    "Now that the strings are ready, it is only a matter of iterating over the verses,\n",
    "matching up the strings, and cross-referencing the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419079 matches made\n",
      "33 matches missed\n"
     ]
    }
   ],
   "source": [
    "bhsa2wlc = []\n",
    "no_match = []\n",
    "\n",
    "for ref_tuple, string_data in verse_strings['bhsa'].items():\n",
    "        \n",
    "    gbi_strings = verse_strings['gbi'][ref_tuple]\n",
    "        \n",
    "    # look for matching strings with pairwise iteration\n",
    "    # if match, save indices for alignment\n",
    "    bhsa_indices = None\n",
    "    gbi_indices = None\n",
    "    for b_str_name, b_str, b_indices in string_data:\n",
    "        for g_str_name, g_str, g_indices in gbi_strings:\n",
    "            if b_str == g_str:\n",
    "                bhsa_indices = b_indices # set indices\n",
    "                gbi_indices = g_indices\n",
    "                break\n",
    "        if bhsa_indices: # break double loop\n",
    "            break\n",
    "    \n",
    "    # -- no match: record a null match --\n",
    "    if not bhsa_indices:\n",
    "        no_match.append([ref_tuple, string_data, gbi_strings])\n",
    "        continue\n",
    "        \n",
    "    # -- match! continue on to alignment maneuver --     \n",
    "    \n",
    "    # remap gbi string indices to be the keys for easy selection\n",
    "    gbi_str2word = {\n",
    "        str_index:word_id for word_id, indices in gbi_indices.items()\n",
    "            for str_index in indices\n",
    "    }\n",
    "    \n",
    "    # finally, make the matches by iterating through \n",
    "    # string indices matched to wordnode, add to set \n",
    "    # to avoid duplicates\n",
    "    for wordnode, str_indices in bhsa_indices.items():\n",
    "        \n",
    "        aligned_gbi = set()\n",
    "        \n",
    "        for si in str_indices:\n",
    "            gbi_word = gbi_str2word[si]            \n",
    "            aligned_gbi.add(gbi_word)\n",
    "                        \n",
    "        # done! save result\n",
    "            \n",
    "        # check to see if WLC word has already been matched\n",
    "        # with previous BHSA word; if so, expand BHSA side\n",
    "        # of the alignment instead\n",
    "        if bhsa2wlc and aligned_gbi.issubset(set(bhsa2wlc[-1][1])):\n",
    "            bhsa2wlc[-1][0].append(wordnode)        \n",
    "        else:\n",
    "            bhsa2wlc.append([[wordnode], sorted(aligned_gbi)])\n",
    "        \n",
    "print(len(bhsa2wlc), 'matches made')\n",
    "print(len(no_match), 'matches missed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 alterations prepared\n"
     ]
    }
   ],
   "source": [
    "# Bridge missing links\n",
    "\n",
    "def find_word_from_index(index_set, index_dict):\n",
    "    \"\"\"Selects a word from string index dict\"\"\"\n",
    "    for word, indices in index_dict.items():\n",
    "        if index_set.issubset(set(indices)):\n",
    "            return word\n",
    "        \n",
    "def build_edits(stringset1, stringset2, debug=False):\n",
    "    \"\"\"Iterate through 2 stringsets and look for the differences\n",
    "    \n",
    "    Needed to manually correct unlinked verses\n",
    "    \"\"\"\n",
    "    \n",
    "    # find closest pairwise match with edit distance\n",
    "    scores = []\n",
    "    for namei, stri, indi in stringset1:\n",
    "        for namej, strj, indj in stringset2:\n",
    "            scores.append((Levenshtein.distance(stri, strj),  (namei, stri, indi), (namej, strj, indj)))\n",
    "    closest_set = sorted(scores)[0]\n",
    "    \n",
    "    # calculate edits necessary\n",
    "    source_set, dest_set = closest_set[1:]\n",
    "    source_str, dest_str = source_set[1], dest_set[1]\n",
    "    source_inds, dest_inds = source_set[2], dest_set[2]\n",
    "    edit_ops = Levenshtein.editops(source_str, dest_str)\n",
    "    \n",
    "    # use edit instructions to find and fix\n",
    "    # offending words, note \"i\" refers to index\n",
    "    alterations = []\n",
    "    for op, source_i, dest_i in edit_ops:\n",
    "        \n",
    "        offending_word = find_word_from_index({source_i}, source_inds)\n",
    "        ow_text = normalize_string(gbi_words[offending_word]['text'])\n",
    "        \n",
    "        # edit op indices are relative to the whole verse string\n",
    "        # in order to relate it to a single word, must \n",
    "        # first find out which index that word starts with in verse string\n",
    "        # and adjust with the difference accordingly\n",
    "        orig_source_i = source_i # keep copy for debug\n",
    "        first_i = source_inds[offending_word][0]\n",
    "        source_i = source_i - first_i\n",
    "        \n",
    "        # apply corrections using indices \n",
    "        if op == 'delete':\n",
    "            mod_text = ow_text[:source_i] + ow_text[source_i+1:]\n",
    "        elif op == 'insert':\n",
    "            ins_char = dest_str[dest_i]\n",
    "            mod_text = ow_text[:source_i] + ins_char + ow_text[source_i:]\n",
    "        elif op == 'replace':\n",
    "            repl_char = dest_str[dest_i]\n",
    "            mod_text= ow_text[:source_i] + repl_char + ow_text[source_i+1:]\n",
    "        \n",
    "        # save corrections\n",
    "        alterations.append({'id':offending_word, 'original': ow_text, 'mod': mod_text})\n",
    "\n",
    "        # provide printout of activity\n",
    "        if debug:\n",
    "            print(op)\n",
    "            print(source_set[0], source_str)\n",
    "            print(dest_set[0], dest_str)\n",
    "        \n",
    "    return alterations\n",
    "   \n",
    "\n",
    "# store edits here\n",
    "gbi_word_alts = []\n",
    "debug = False\n",
    "    \n",
    "for nm in no_match:\n",
    "    verse, bhsa_str, gbi_str = nm\n",
    "    alterations = build_edits(gbi_str, bhsa_str, debug)\n",
    "    if debug:\n",
    "        print(verse)\n",
    "        print(alterations)\n",
    "        print()\n",
    "        print('-'*60)\n",
    "    else:\n",
    "        gbi_word_alts.extend(alterations)\n",
    "\n",
    "if not debug:\n",
    "    print(len(gbi_word_alts), 'alterations prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 10140020171, 'original': 'צבויימ', 'mod': 'צביימ'},\n",
       " {'id': 90250180131, 'original': 'עשוית', 'mod': 'עשויות'},\n",
       " {'id': 100180080051, 'original': 'נפצת', 'mod': 'נפצית'},\n",
       " {'id': 100180120062, 'original': 'לוא', 'mod': 'לא'},\n",
       " {'id': 120160060152, 'original': 'אדומימ', 'mod': 'אדמימ'},\n",
       " {'id': 120190230072, 'original': 'רב', 'mod': 'רכב'},\n",
       " {'id': 230300050031, 'original': 'הביש', 'mod': 'הבאיש'},\n",
       " {'id': 230420240042, 'original': 'משסה', 'mod': 'משוסה'},\n",
       " {'id': 240050060221, 'original': 'משובותי', 'mod': 'משבותי'},\n",
       " {'id': 240150110061, 'original': 'שריתי', 'mod': 'שרית'},\n",
       " {'id': 240180160051, 'original': 'שריקות', 'mod': 'שריקת'},\n",
       " {'id': 240420200031, 'original': 'התעיתמ', 'mod': 'התעתמ'},\n",
       " {'id': 240480050042, 'original': 'לוחית', 'mod': 'לחית'},\n",
       " {'id': 260320320052, 'original': 'י', 'mod': 'ו'},\n",
       " {'id': 260400310013, 'original': 'ו', 'mod': 'יו'},\n",
       " {'id': 260430110122, 'original': 'ו', 'mod': 'יו'},\n",
       " {'id': 260440240091, 'original': 'ישפטו', 'mod': 'ישפט'},\n",
       " {'id': 191400100062, 'original': 'מו', 'mod': 'ומו'},\n",
       " {'id': 180060290072, 'original': 'שובו', 'mod': 'שבו'},\n",
       " {'id': 180260140131, 'original': 'גבורותי', 'mod': 'גבורתי'},\n",
       " {'id': 180380120071, 'original': 'ה', 'mod': ''},\n",
       " {'id': 200130200021, 'original': 'הולכ', 'mod': 'הלכ'},\n",
       " {'id': 200190070131, 'original': 'ל', 'mod': ''},\n",
       " {'id': 200190070132, 'original': 'ו', 'mod': ''},\n",
       " {'id': 80030140031, 'original': 'מרגלותי', 'mod': 'מרגלתי'},\n",
       " {'id': 210050170201, 'original': 'חיי', 'mod': 'חי'},\n",
       " {'id': 270020380041, 'original': 'דירינ', 'mod': 'דארינ'},\n",
       " {'id': 270020410062, 'original': 'הנ', 'mod': 'הינ'},\n",
       " {'id': 270030180093, 'original': 'כ', 'mod': 'יכ'},\n",
       " {'id': 270040290172, 'original': 'כ', 'mod': 'יכ'},\n",
       " {'id': 270050070263, 'original': 'א', 'mod': 'ה'},\n",
       " {'id': 270050160071, 'original': 'תיכול', 'mod': 'תכול'},\n",
       " {'id': 270050160233, 'original': 'א', 'mod': 'ה'},\n",
       " {'id': 270100190102, 'original': 'ב', 'mod': 'כ'},\n",
       " {'id': 160050090022, 'original': 'אומר', 'mod': 'אמר'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbi_word_alts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
