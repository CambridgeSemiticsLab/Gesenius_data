{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build English Verb Dataset\n",
    "\n",
    "In this notebook, we build a translation dataset based on NIV and ESV translation\n",
    "alignments provided by GBI. The GBI data, which uses an underlying WLC Hebrew text\n",
    "has already been aligned to the Amsterdam BHSA Hebrew dataset in \n",
    "[GBI_alignment_wrangling.ipynb](GBI_alignment_wrangling.ipynb). We can thus take\n",
    "advantage of both databases and their associated data when building our dataset here.\n",
    "\n",
    "In the dataset we'll attempt to parse the English text so that the syntax and (especially)\n",
    "the verbal forms can be analyzed alongside the Hebrew grammar. We'll start out with \n",
    "Spacy for the English parsings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b title=\"local commit\">TF-app:</b> <span title=\"#113c0687cfce3077734dac1844d244d20f4ace6f offline under ~/text-fabric-data\">~/text-fabric-data/annotation/app-bhsa/code</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv1.6 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/bhsa/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"r1.2 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/phono/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"r1.2 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/parallels/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 8.4.0</a>, <a target=\"_blank\" href=\"https://github.com/annotation/app-bhsa\" title=\"bhsa TF-app\">app-bhsa</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br><b>Data:</b> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/0_home\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/writing/hebrew.html\" title=\"How TF features represent text\">Character table</a>, <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/0_home\" title=\"BHSA feature documentation\">Feature docs</a><br><b>Features:</b><br><details><summary><b>Parallel Passages</b></summary><b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"~/text-fabric-data/etcbc/parallels/tf/c/crossref.tf\">crossref</a></i></b><br></details><details><summary><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b></summary><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/book.tf\">book</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book@ll\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/book@am.tf\">book@ll</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/chapter\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/chapter.tf\">chapter</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/code\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/code.tf\">code</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/det\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/det.tf\">det</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/domain\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/domain.tf\">domain</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/freq_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/freq_lex.tf\">freq_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/function\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/function.tf\">function</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_cons.tf\">g_cons</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_cons_utf8.tf\">g_cons_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_lex.tf\">g_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_lex_utf8.tf\">g_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_word.tf\">g_word</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_word_utf8.tf\">g_word_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gloss\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/gloss.tf\">gloss</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/gn.tf\">gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/label\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/label.tf\">label</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/language\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/language.tf\">language</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/lex.tf\">lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/lex_utf8.tf\">lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ls\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/ls.tf\">ls</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nametype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nametype.tf\">nametype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nme\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nme.tf\">nme</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nu.tf\">nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/number\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/number.tf\">number</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/otype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pargr\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pargr.tf\">pargr</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pdp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pdp.tf\">pdp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pfm\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pfm.tf\">pfm</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs.tf\">prs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_gn.tf\">prs_gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_nu.tf\">prs_nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_ps.tf\">prs_ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/ps.tf\">ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere.tf\">qere</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer.tf\">qere_trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer_utf8.tf\">qere_trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_utf8.tf\">qere_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rank_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/rank_lex.tf\">rank_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rela\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/rela.tf\">rela</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/sp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/sp.tf\">sp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/st\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/st.tf\">st</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/tab\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/tab.tf\">tab</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/trailer.tf\">trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/trailer_utf8.tf\">trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/txt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/txt.tf\">txt</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/typ\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/typ.tf\">typ</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/uvf\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/uvf.tf\">uvf</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbe\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vbe.tf\">vbe</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vbs.tf\">vbs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/verse\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/verse.tf\">verse</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/voc_lex.tf\">voc_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/voc_lex_utf8.tf\">voc_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vs.tf\">vs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vt.tf\">vt</a><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/mother\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/mother.tf\">mother</a></i></b><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/oslots\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/oslots.tf\">oslots</a></i></b><br></details><details><summary><b>Phonetic Transcriptions</b></summary><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/c/phono.tf\">phono</a><br><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/c/phono_trailer.tf\">phono_trailer</a><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/server/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 0.1rem;\n",
       "    margin: 0.1rem;\n",
       "    direction: ltr;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1rem 0rem;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".section {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--section);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  0.5rem 0.1rem 0.1rem 0.1rem;\n",
       "    margin: 0.8rem 0.1rem 0.1rem 0.1rem;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 0.2rem;\n",
       "    margin-left: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 0.2rem;\n",
       "    margin-right: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -1.2rem;\n",
       "    margin-left: 1rem;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3rem;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 0.1rem;\n",
       "    margin-left: 0.1rem;\n",
       "    padding: 0.1rem 0.1rem;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 2rem;\n",
       "\tpadding: 1rem;\n",
       "\tborder: 0.1rem solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--section:            hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         0.15rem;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   0.1rem;\n",
       "  --border-width0:      0.1rem;\n",
       "  --border-width1:      0.15rem;\n",
       "  --border-width2:      0.2rem;\n",
       "  --border-width3:      0.3rem;\n",
       "  --border-width4:      0.25rem;\n",
       "  --border-width-plain: 0.1rem;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 0.2rem ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 0.2rem ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.3rem;\n",
       "  padding: 0.2rem;\n",
       "  margin: 0.2rem;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "from pathlib import Path\n",
    "from tf.app import use\n",
    "from bidict import bidict # bidirectional dictionary\n",
    "\n",
    "# custom modules \n",
    "import tf_tools\n",
    "from gbi_functions import id2ref\n",
    "from positions import PositionsTF\n",
    "\n",
    "# organize pathways\n",
    "PROJ_DIR = Path.home().joinpath('github/CambridgeSemiticsLab/translation_traditions_HB')\n",
    "PRIVATE_DATA = PROJ_DIR.joinpath('data/_private_')\n",
    "GBI_DATA_DIR = PRIVATE_DATA.joinpath('GBI_alignment')\n",
    "VERB_DIR = PRIVATE_DATA.joinpath('verb_data')\n",
    "\n",
    "# load GBI data\n",
    "gbi_niv = json.loads(GBI_DATA_DIR.joinpath('niv84.ot.alignment.json').read_text())\n",
    "gbi_esv = json.loads(GBI_DATA_DIR.joinpath('esv.ot.alignment.json').read_text())\n",
    "\n",
    "# load BHSA / GBI Alignment\n",
    "bhsa2gbi = json.loads(GBI_DATA_DIR.joinpath('bhsa2gbi.json').read_text())\n",
    "\n",
    "# load BHSA data and methods\n",
    "bhsa = use('bhsa')\n",
    "api = bhsa.api\n",
    "F, E, T, L = api.F, api.E, api.T, api.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some dictionaries for convenient word data access\n",
    "# see 'Dict demos' in next cells for quick intro to the resulting dict structures\n",
    "\n",
    "sources = (('niv', gbi_niv), ('esv', gbi_esv))\n",
    "word_data = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "verse2words = collections.defaultdict(lambda: collections.defaultdict(list)) \n",
    "linkbyid = collections.defaultdict(list) # list of 2-tuples, each containing word IDs\n",
    "id2link = collections.defaultdict(dict) # select a link based on a single ID \n",
    "\n",
    "for name, source in sources:\n",
    "    for verse in source:\n",
    "        \n",
    "        # unpack words for processing\n",
    "        trans_words =  verse['translation']['words']\n",
    "        manu_words = verse['manuscript']['words']\n",
    "        \n",
    "        # map translation word data\n",
    "        for w in trans_words:\n",
    "            ref_tuple = id2ref(w['id'], 'translation')\n",
    "            verse2words[name][ref_tuple].append(w['id'])\n",
    "            word_data[name][w['id']] = w\n",
    "        \n",
    "        # map WLC word data\n",
    "        # arbitrarily use the copy stored under NIV\n",
    "        if name == 'niv':\n",
    "            for w in manu_words:\n",
    "                ref_tuple = id2ref(w['id'])\n",
    "                verse2words['wlc'][ref_tuple].append(w['id'])\n",
    "                word_data['wlc'][w['id']] = w\n",
    "                \n",
    "        # map links to word ids\n",
    "        # the alignment data just contains indices pointing\n",
    "        # to the various lists, so these have to be used to \n",
    "        # identify the specific word in question\n",
    "        for wlc_indices, trans_indices in verse['links']:\n",
    "            wlc_ids = tuple(manu_words[i]['id'] for i in wlc_indices)\n",
    "            trans_ids = tuple(sorted(trans_words[i]['id'] for i in trans_indices))\n",
    "            linkbyid[name].append((wlc_ids, trans_ids))\n",
    "            for wid in wlc_ids:\n",
    "                id2link[name][wid] = trans_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 10010010021,\n",
       " 'altId': 'בָּרָ֣א\\u200e-1',\n",
       " 'text': 'בָּרָ֣א\\u200e',\n",
       " 'strong': 'H1254',\n",
       " 'gloss': 'he created',\n",
       " 'gloss2': '创造',\n",
       " 'lemma': 'ברא_1',\n",
       " 'pos': 'verb',\n",
       " 'morph': 'vqp3ms'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data['wlc'][10010010021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1001001005,\n",
       " 'altId': 'created-1',\n",
       " 'text': 'created',\n",
       " 'transType': 'k',\n",
       " 'isPunc': False,\n",
       " 'isPrimary': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data['niv'][1001001005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1001001001,\n",
       " 1001001002,\n",
       " 1001001003,\n",
       " 1001001004,\n",
       " 1001001005,\n",
       " 1001001006,\n",
       " 1001001007,\n",
       " 1001001008,\n",
       " 1001001009,\n",
       " 1001001010,\n",
       " 1001001011]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse2words['niv'][('Genesis', 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10010010021,) (1001001005,)\n"
     ]
    }
   ],
   "source": [
    "for wlc_ids, trans_ids in linkbyid['niv']:\n",
    "    if 10010010021 in wlc_ids:\n",
    "        print(wlc_ids, trans_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001001005,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB: similar to above, however\n",
    "# the link is only 1-to-X\n",
    "# so some parts of the left side of the link could be missing\n",
    "id2link['niv'][10010010021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-sample verbs\n",
    "\n",
    "The basis of the dataset is verbs. The jumping-off point is the BHSA syntax data. Thus what\n",
    "we do is assemble the dataset by the BHSA verbs.\n",
    "\n",
    "First we get a one-to-one verb mapping between BHSA and GBI Hebrew (WLC). We can use the GBI hebrew\n",
    "links to select the correct words in the translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main stipulation for the selection is agreement between BHSA and WLC on \n",
    "the classification of a word as a verb (e.g. טוֹב); for BHSA classification, \n",
    "we use a contextual definition (phrase-dependent) which classifies whether \n",
    "the word is behaving as a verb in context (e.g. participles)\n",
    "\n",
    "Since some alignments between BHSA and WLC are `many-to-N` or `N-to-many`, \n",
    "we also filter out any of these non-verbal words. This leaves us with\n",
    "a 1-to-1 alignment, so that 1 verb in BHSA equals 1 verb in WLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4427 verbs do not match requirements\n",
      "68826 selected for building dataset\n"
     ]
    }
   ],
   "source": [
    "# track where BHSA and WLC disagree on the classification of a verb\n",
    "no_match = {\n",
    "    'disagree': [],\n",
    "}\n",
    "\n",
    "verb_bhsa2gbi = {}\n",
    "\n",
    "# find 1-to-1 matches of BHSA and WLC verbs\n",
    "for bhsa_nodes, gbi_ids in bhsa2gbi:\n",
    "    \n",
    "    # filter out non-verbs from the links\n",
    "    bhsa_verbs = [w for w in bhsa_nodes if F.pdp.v(w) == 'verb'] \n",
    "    wlc_verbs = [w for w in gbi_ids if word_data['wlc'][w]['pos'] == 'verb']\n",
    "    data = (T.text(bhsa_nodes), T.sectionFromNode(bhsa_nodes[0]), bhsa_nodes, gbi_ids) # track null matches\n",
    "    \n",
    "    # one case, Jer 51:3, has a double verb mapping caused by \n",
    "    # ידרך ידרך, which BHSA maps to a single word node, and gbi \n",
    "    # keeps as 2 words; we disambig that here and keep only \n",
    "    # first gbi word\n",
    "    if bhsa_verbs and bhsa_verbs[0] == 262780:\n",
    "         wlc_verbs = wlc_verbs[:1]\n",
    "    \n",
    "    # skip non-verbal contexts\n",
    "    if not bhsa_verbs + wlc_verbs:\n",
    "        continue\n",
    "    \n",
    "    # track disagreements between 2 sources\n",
    "    elif (bhsa_verbs and not wlc_verbs) or (wlc_verbs and not bhsa_verbs):\n",
    "        no_match['disagree'].append(data)\n",
    "    \n",
    "    # store result both ways: bhsa 2 wlc, wlc 2 bhsa\n",
    "    elif len(bhsa_verbs) == 1 and len(wlc_verbs) == 1:\n",
    "        \n",
    "        # make a subset selection of verbs\n",
    "        bhsa_verb, wlc_verb = bhsa_verbs[0], wlc_verbs[0]\n",
    "        verb_bhsa2gbi[bhsa_verb] = word_data['wlc'][wlc_verb]\n",
    "        \n",
    "    \n",
    "    # or there's a problem...\n",
    "    else:\n",
    "        raise Exception(f'Misalignment at {data}')\n",
    "        \n",
    "print(sum(len(v) for v in no_match.values()), 'verbs do not match requirements')\n",
    "print(len(verb_bhsa2gbi), 'selected for building dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB that the ~4.4k verbs in disagreement is because we use contextual parts of speech\n",
    "from the BHSA dataset. The GBI dataset does not seem to be as sensitive to context for\n",
    "pos. A large proportion of these cases are participles used as nouns rather than verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('פְּקֻדֵיהֶ֖ם ', ('Numbers', 1, 39), [70153], [40010390011, 40010390012]),\n",
       " ('יֹצֵ֥א ', ('Numbers', 1, 40), [70183], [40010400141]),\n",
       " ('פְּקֻדֵיהֶ֖ם ', ('Numbers', 1, 41), [70185], [40010410011, 40010410012]),\n",
       " ('יֹצֵ֥א ', ('Numbers', 1, 42), [70214], [40010420141]),\n",
       " ('פְּקֻדֵיהֶ֖ם ', ('Numbers', 1, 43), [70216], [40010430011, 40010430012]),\n",
       " ('פְּקֻדִ֡ים ', ('Numbers', 1, 44), [70229], [40010440022]),\n",
       " ('פְּקוּדֵ֥י ', ('Numbers', 1, 45), [70250], [40010450031]),\n",
       " ('פְּקֻדִ֔ים ', ('Numbers', 1, 46), [70271], [40010460032]),\n",
       " ('פְקֻדֵיהֶ֑ם ', ('Numbers', 2, 4), [70477], [40020040022, 40020040023]),\n",
       " ('פְקֻדָ֑יו ', ('Numbers', 2, 6), [70502], [40020060022, 40020060023])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_match['disagree'][500:510]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the dataset for later processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing English with Spacy\n",
    "\n",
    "For understanding the basics of Spacy, see:\n",
    "https://spacy.io/usage/linguistic-features\n",
    "\n",
    "For each verb in the `select_verbs` dictionary, we retrieve its verse text in a\n",
    "given translation. The translated text is parsed by Spacy, which supplies us with\n",
    "a dependency tree, parts of speech, and verb tenses for the English side of things.\n",
    "\n",
    "For Spacy tags used with the model of choice, see:\n",
    "https://github.com/explosion/spacy-models/releases//tag/en_core_web_sm-2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "from spacy.util import filter_spans # filter overlaps; nice tip: https://stackoverflow.com/a/63303480/8351428"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build links between BHSA and English translations via WLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-verbs selected for esv: 66837\n",
      "\tn-verbs unlinked: 1989\n",
      "n-verbs selected for niv: 65455\n",
      "\tn-verbs unlinked: 3371\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "\n",
    "english_verbs = collections.defaultdict(dict)\n",
    "not_linked = collections.defaultdict(list)\n",
    "\n",
    "for trans in ('esv', 'niv'):\n",
    "    \n",
    "    for bhsa_node, wlc_word in verb_bhsa2gbi.items():\n",
    "        try:\n",
    "            english_verbs[trans][bhsa_node] = id2link[trans][wlc_word['id']]\n",
    "        except: \n",
    "            not_linked[trans].append(wlc_word)\n",
    "        \n",
    "    print(f'n-verbs selected for {trans}: {len(english_verbs[trans])}')\n",
    "    print(f'\\tn-verbs unlinked: {len(not_linked[trans])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse English Data within its Verse Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esv has 21264 verses to parse\n",
      "niv has 21251 verses to parse\n"
     ]
    }
   ],
   "source": [
    "# to avoid double-parsing verses, we map\n",
    "# all cases needed to be parsed to a single\n",
    "# parsed verse\n",
    "\n",
    "# for each translation, gather the verse labels\n",
    "# that need to be parsed\n",
    "\n",
    "versestoparse = collections.defaultdict(set)\n",
    "for trans in english_verbs:\n",
    "    for bhsa_node, trans_words in english_verbs[trans].items():\n",
    "        ref = id2ref(trans_words[0], 'translation')\n",
    "        versestoparse[trans].add(ref)\n",
    "        \n",
    "    print(f'{trans} has {len(versestoparse[trans])} verses to parse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply raw Spacy parsing to all relevant verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the Spacy processor as well as some customized attributes\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "Span.set_extension('tam_tag', default='', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(gbi_words):\n",
    "    \"\"\"Prepare GBI pre-tokenized words for Spacy Doc construction\"\"\"\n",
    "    gbi_text = [w['text'] for w in gbi_words]\n",
    "    doc = Doc(nlp.vocab, words=gbi_text)\n",
    "    return doc\n",
    "\n",
    "# edit default tokenizer in order to feed in \n",
    "# already-tokenized GBI words\n",
    "nlp.tokenizer = process_tokens\n",
    "\n",
    "def parse_verse(verse_tuple, translation):\n",
    "    \"\"\"Parse translation verse with Spacy.\"\"\"\n",
    "    tokens = [word_data[translation][w] for w in verse2words[translation][verse_tuple]]\n",
    "    parsed_doc = nlp(tokens, translation) # magic happens here\n",
    "    return parsed_doc\n",
    "\n",
    "def parse_verses(transdict):\n",
    "    \"\"\"Iterate through all verses and parse them.\n",
    "    \n",
    "    Args:\n",
    "        versedict: dict with structure of e.g. {'niv': set(('Genesis', 1, 1)...}}\n",
    "    Returns:\n",
    "        dict w/ structure of e.g. {'niv': {('Genesis', 1, 1): Spacy.Doc}}\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed_verses = collections.defaultdict(dict)\n",
    "    \n",
    "    bhsa.indent(0, reset=True)\n",
    "    bhsa.info(f'Parsing translations...')\n",
    "    \n",
    "    for translation, verse_set in transdict.items():\n",
    "    \n",
    "        # it takes a long time so we time it\n",
    "        bhsa.indent(1)\n",
    "        bhsa.info(f'Beginning {translation}...')\n",
    "        bhsa.indent(2)\n",
    "    \n",
    "        # parse the verse and put Spacy.Doc in a dict\n",
    "        for i, ref_tuple in enumerate(verse_set):\n",
    "            \n",
    "            if i % 5000 == 0 and i != 0:\n",
    "                bhsa.info(f'done with verse {i}')\n",
    "                \n",
    "            parsed_verses[translation][ref_tuple] = parse_verse(ref_tuple, translation)\n",
    "            \n",
    "        bhsa.indent(1)\n",
    "        bhsa.info('done!')\n",
    "    \n",
    "    return parsed_verses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute the parser for all verses. This will take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle here to run fresh parsings\n",
    "parsed_verses_file = '_private_/parsings/parsed_verses.pickle'\n",
    "if False:\n",
    "    parsed_verses = parse_verses(versestoparse)\n",
    "    with open(parsed_verses_file, 'wb') as outfile:\n",
    "        pickle.dump(parsed_verses, outfile)\n",
    "else:\n",
    "    with open(parsed_verses_file, 'rb') as infile:\n",
    "        parsed_verses = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In the beginning God created the heavens and the earth . "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_verses['niv'][('Genesis', 1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Matcher rules for advanced TAM tags\n",
    "\n",
    "Spacy parses raw strings into tags and dependencies. For verbs we are particularly\n",
    "interested in tense, aspect, and modality (TAM). The default tags are not very informative with \n",
    "regard to TAM. But we can also achieve these labels ourselves by adding some additional rules.\n",
    "\n",
    "We will use Spacy's Matcher class for this, alongside the parser:\n",
    "\n",
    "https://spacy.io/usage/rule-based-matching\n",
    "\n",
    "To-do list of primary English tense constructions, curated from:\n",
    "\n",
    "https://en.wikipedia.org/wiki/English_verbs#Expressing_tenses,_aspects_and_moods\n",
    "\n",
    "```\n",
    "simple present            writes\n",
    "simple past               wrote\n",
    "present progressive       is writing\n",
    "past progressive          was writing\n",
    "present perfect           has written\n",
    "past perfect              had written\n",
    "present perf. progress.   has been writing\n",
    "past perf. progress.      had been writing\n",
    "future                    will write\n",
    "future perfect            will have written\n",
    "future perf. progress.    will have been writing\n",
    "```\n",
    "\n",
    "secondary constructions:\n",
    "\n",
    "```\n",
    "imperative               write\n",
    "future-in-past           would write\n",
    "do-support               does write\n",
    "be-going-to future       is going to write\n",
    "```\n",
    "\n",
    "Later on, we can consider dividing these constructions up into 3 columns -- 1 each for \n",
    "tense, aspect, and modality. If a construction contributes to one of these categories,\n",
    "the column gets filled. Otherwise it is left empty. \n",
    "\n",
    "```\n",
    "\"has been writing\"\n",
    "\n",
    "tense           aspect            modality\n",
    "-----           ------             ------\n",
    "past      perfect progressive\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set of rules to match tense-aspect-modality construtions in English\n",
    "# overlapping results will be filtered out and the longest matching span\n",
    "# will be kept in its place\n",
    "\n",
    "# these patterns can be inserted between verb auxiliaries\n",
    "# and their heads to represent any number of interrupting\n",
    "# adverbial modifiers\n",
    "advb_pronouns = {'TAG': {'IN':['RB', 'PRP']}, 'OP': '*'}\n",
    "advbs = {'TAG': {'IN':['RB']}, 'OP': '*'}\n",
    "    \n",
    "tam_rules = [\n",
    "    (\n",
    "        'present', \n",
    "        [\n",
    "            {'TAG':{'IN':['VBZ', 'VBP']}, 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'present progressive', \n",
    "        [\n",
    "            {'TAG': {'IN':['VBZ', 'VBP']}, 'LEMMA':'be'},\n",
    "            advb_pronouns,\n",
    "            {'TAG':'VBG', 'LEMMA': {'NOT_IN':['go']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'present perfect',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBZ', 'VBP']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'present perfect progressive',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBZ', 'VBP']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'LEMMA': 'be'},\n",
    "            {'TAG': 'VBG'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'past',\n",
    "        [\n",
    "            {'TAG': 'VBD', 'DEP': {'NOT_IN':['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'past perfect',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBD']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'past perfect progressive',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBD']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'LEMMA': 'be'},\n",
    "            {'TAG': 'VBG'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'future perfect',\n",
    "        [\n",
    "            {'TAG': 'MD', 'LEMMA': 'will'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': {'IN': ['VB']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'future perfect progressive',\n",
    "        [\n",
    "            {'TAG': 'MD', 'LEMMA': 'will'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': {'IN': ['VB']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'LEMMA': 'be'},\n",
    "            {'TAG': 'VBG'},\n",
    "        ]\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        'past progressive',\n",
    "        [\n",
    "            {'TAG':'VBD', 'LEMMA':'be'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBG'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'future',\n",
    "        [\n",
    "            {'TAG': 'MD', 'LEMMA': 'will'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VB', 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'future-in-past', # habitual?\n",
    "        [\n",
    "            {'LOWER': 'would', 'DEP': {'IN': ['aux']}},\n",
    "            advb_pronouns,\n",
    "            {'TAG':'VB'}\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'do-support present',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBZ', 'VBP']}, 'LEMMA': 'do'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VB'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'past perfect (did)',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBD']}, 'LEMMA': 'do'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VB'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'be-going-to future',\n",
    "        [\n",
    "            {'TAG': {'IN':['VBZ', 'VBP']}, 'LEMMA':'be'}, \n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBG', 'LEMMA': 'go'},\n",
    "            {'TAG': 'TO'},\n",
    "            {'TAG': 'VB'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'modal',\n",
    "        [\n",
    "            {'TAG': {'IN':['VB', 'MD']}, 'lower': {'IN':['let', 'may', 'shall', 'must']}},\n",
    "            {'TAG': {'NOT_IN':['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']}, 'OP': '*'},\n",
    "            {'TAG': 'VB'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'present participle', \n",
    "        [\n",
    "            {'TAG': 'VBG', 'DEP': {'NOT_IN':['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'present participle',\n",
    "        [\n",
    "            {'TAG': 'JJ', 'LOWER':{'REGEX':'^.+ing$'}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'to infinitive',\n",
    "        [\n",
    "            {'TAG': 'TO'},\n",
    "            {'TAG':'VB'}\n",
    "        ]\n",
    "    ),\n",
    "    \n",
    "    # add another modal category:\n",
    "    # \"Let him go up\"\n",
    "    # i.e. \"Let/may ... verb\"\n",
    "  \n",
    "# This pattern as-is is too over-active in matches\n",
    "# need to find another way to match it\n",
    "#     (\n",
    "#         'imperative',\n",
    "#         [\n",
    "#             {'TAG': 'VB', 'DEP':{'NOT_IN':['aux']}},\n",
    "#         ]\n",
    "#     ),\n",
    "    \n",
    "]\n",
    "\n",
    "# def on_match(matcher, doc, mid, matches):\n",
    "#     for match in matches:\n",
    "#         begin, end = match[1:]\n",
    "#         tam_matches[(begin, end)].add(match)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add all rules to the matcher object\n",
    "for tag, rules in tam_rules:\n",
    "    matcher.add(tag, None, rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esv n-spans: 20870\n",
      "niv n-spans: 20730\n"
     ]
    }
   ],
   "source": [
    "# for every verse isolate the set of relevant spans\n",
    "# which match the TAM rules and map to verses\n",
    "verse2spans = collections.defaultdict(dict)\n",
    "for trans, ref_tuples in parsed_verses.items():\n",
    "    for ref_tuple, spacy_doc in ref_tuples.items():\n",
    "        matches = matcher(spacy_doc)\n",
    "        \n",
    "        # retrieve Spacy Span objects\n",
    "        # and give them TAM tags\n",
    "        spans = []\n",
    "        for m_id, start, end in matches:\n",
    "            span = spacy_doc[start:end]\n",
    "            span._.tam_tag = nlp.vocab.strings[m_id]\n",
    "            spans.append(span)\n",
    "        \n",
    "        # filter out overlapping spans and keep \n",
    "        # only the longest strings\n",
    "        filtered_spans = filter_spans(spans)\n",
    "        \n",
    "        # save positive matches; unmatched verses will\n",
    "        # be recognized later\n",
    "        if filtered_spans:\n",
    "            verse2spans[trans][ref_tuple] = filtered_spans\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    print(f'{trans} n-spans: {len(verse2spans[trans])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export a set for inspection\n",
    "# inspect = ''\n",
    "# for verse_ref, spans in verse2spans['niv'].items():\n",
    "#     inspect += str(verse_ref) + '\\n'\n",
    "#     inspect += str(parsed_verses['niv'][verse_ref]) + '\\n'\n",
    "#     for span in spans:\n",
    "#         inspect += f'\\t{span} -> {span._.tam_tag}\\n'\n",
    "        \n",
    "#     inspect += '\\n\\n'\n",
    "\n",
    "# Path('inspect_tam.txt').write_text(inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in L.d(1414423,'word'):\n",
    "#     print(w, T.text(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in [w for w in verse2words['wlc'][('Genesis', 3, 11)]]:\n",
    "    \n",
    "#     print(w, word_data['wlc'][w]['text'])\n",
    "#     print(id2link['niv'].get(w))\n",
    "#     print(' '.join(word_data['niv'][w]['text'] for w in id2link['niv'].get(w, [])))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens(doc):\n",
    "    for token in doc:\n",
    "        print(token)\n",
    "        print('  ', token.lemma_)\n",
    "        print('  ', token.pos_)\n",
    "        print('  ', token.tag_)\n",
    "        print('  ', token.dep_)\n",
    "        print('  ', token.shape_)\n",
    "        \n",
    "#display_tokens(parsed_verses['niv'][('Genesis', 1, 24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_tokens(nlp2('Let there be an expanse between the waters to separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[said, Let there be, was]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse2spans['niv'][('Genesis', 1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conjunction, subordinating or preposition'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('IN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link spans to verbs\n",
    "\n",
    "We will now attempt to re-link the spans with the verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_span(trans_words, spans, verse_words):\n",
    "    \"\"\"Match given words with its TAM span.\n",
    "    \n",
    "    The links are made based on set overlap.\n",
    "    \"\"\"\n",
    "    for span in spans:\n",
    "        span_words = set(verse_words[span.start:span.end])\n",
    "        if set(trans_words) & span_words:\n",
    "            return span\n",
    "\n",
    "verse_inspect = collections.defaultdict(lambda: collections.defaultdict(str)) # for exporting an inspection document\n",
    "bhsa2eng = collections.defaultdict(dict)\n",
    "\n",
    "for trans, bhsa_nodes in english_verbs.items():\n",
    "    \n",
    "    for bhsa_node, eng_words in bhsa_nodes.items():\n",
    "\n",
    "        inspect = '' # for debugging and inspection\n",
    "        \n",
    "        verse_ref = id2ref(eng_words[0], 'translation')\n",
    "        eng_text = ' '.join(word_data[trans][w]['text'] for w in eng_words)\n",
    "\n",
    "        # try to retrieve span links with advanced TAM tags\n",
    "        verse_words = verse2words[trans][verse_ref]\n",
    "        spans = verse2spans[trans].get(verse_ref, [])\n",
    "        span_match = trans_to_span(eng_words, spans, verse_words) or ''\n",
    "        if span_match:\n",
    "            tam_tag = span_match._.tam_tag\n",
    "        else:\n",
    "            tam_tag = ''\n",
    "        \n",
    "        # retrieve basic parsings\n",
    "        raw_tokens = []\n",
    "        for i, token in enumerate(parsed_verses[trans][verse_ref]):\n",
    "            if verse_words[i] in eng_words:\n",
    "                raw_tokens.append(token)\n",
    "                \n",
    "        vb_tokens = [t for t in raw_tokens if t.tag_.startswith('VB')]\n",
    "            \n",
    "        # save the data\n",
    "        data = {\n",
    "            'words': eng_text,\n",
    "            'tags': '|'.join(t.tag_ for t in raw_tokens),\n",
    "            'vb_tags': '|'.join(t.tag_ for t in vb_tokens),\n",
    "            'TAM_cx': tam_tag,\n",
    "            'TAM_span': f'{span_match}',\n",
    "        }\n",
    "        \n",
    "        bhsa2eng[trans][bhsa_node] = data\n",
    "            \n",
    "        # add strings to inspection file\n",
    "        if span_match:\n",
    "            verse_inspect[trans][verse_ref] += f'\\tMATCH: {eng_text}\\n'\n",
    "            verse_inspect[trans][verse_ref] += f'\\t\\t{span_match} -> {span_match._.tam_tag}\\n'\n",
    "        else:\n",
    "            verse_inspect[trans][verse_ref] += f'\\tMISS: {eng_text}\\n'\n",
    "            verse_inspect[trans][verse_ref] += f'\\t\\t{eng_text}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "for trans, trans_data  in bhsa2eng.items():\n",
    "    trans_file = VERB_DIR.joinpath(f'bhsa2{trans}.json')\n",
    "    with open(trans_file, 'w') as outfile:\n",
    "        json.dump(trans_data, outfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # export inspection file\n",
    "    write = ''\n",
    "    inspect_file = PRIVATE_DATA.joinpath(f'debugging/{trans}_inspect.txt')\n",
    "    for verse, message in verse_inspect[trans].items():\n",
    "        write += '{} {}:{}'.format(*verse) + '\\n'\n",
    "        write += str(parsed_verses[trans][verse]) + '\\n'\n",
    "        write += message\n",
    "        write += '\\n'\n",
    "    inspect_file.write_text(write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the bhsa2wlc dataset\n",
    "wlc_verbdataset_path = VERB_DIR.joinpath('bhsa2wlc.json') \n",
    "with open(wlc_verbdataset_path, 'w') as outfile:\n",
    "    json.dump(verb_bhsa2gbi, outfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save these resulting dicts for later convenient use\n",
    "\n",
    "# save_files = [\n",
    "#     ('word_data', word_data),\n",
    "#     ('verse2words', {k:{tuple(k2):v2 for k2,v2 in v.items()} for k,v in verse2words.items()}),\n",
    "#     ('id_links', linkbyid),\n",
    "# ]\n",
    "\n",
    "# for filename, data in save_files:\n",
    "#     filepath = GBI_DATA_DIR.joinpath(filename+'.json')\n",
    "#     with open(filepath, 'w') as outfile:\n",
    "#         json.dump(data, outfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export the dataset\n",
    "# dataset_path = PROJ_DIR.joinpath('data/_private_/translation_dataset.csv')\n",
    "\n",
    "# data_df.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "Scratch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write = ''\n",
    "# for verse, cases in verse_sees.items():\n",
    "#     parsed_verse = parsed_verses['niv'][verse]\n",
    "#     ref_str = '{} {}:{}'.format(*verse)\n",
    "#     write += ref_str + '\\n'\n",
    "#     write += str(parsed_verse) + '\\n'\n",
    "#     for case in cases:\n",
    "#         write += case\n",
    "#     write += '\\n'\n",
    "    \n",
    "# Path('see_cases.txt').write_text(write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
