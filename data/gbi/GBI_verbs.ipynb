{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build English Verb Dataset\n",
    "\n",
    "In this notebook, we build a translation dataset based on NIV and ESV translation\n",
    "alignments provided by GBI. The GBI data, which uses an underlying WLC Hebrew text\n",
    "has already been aligned to the Amsterdam BHSA Hebrew dataset in \n",
    "[GBI_alignment_wrangling.ipynb](GBI_alignment_wrangling.ipynb). We can thus take\n",
    "advantage of both databases and their associated data when building our dataset here.\n",
    "\n",
    "In the dataset we'll attempt to parse the English text so that the syntax and (especially)\n",
    "the verbal forms can be analyzed alongside the Hebrew grammar. We'll start out with \n",
    "Spacy for the English parsings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b title=\"local commit\">TF-app:</b> <span title=\"#113c0687cfce3077734dac1844d244d20f4ace6f offline under ~/text-fabric-data\">~/text-fabric-data/annotation/app-bhsa/code</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"rv1.6 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/bhsa/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"r1.2 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/phono/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local release\">data:</b> <span title=\"r1.2 offline under ~/text-fabric-data\">~/text-fabric-data/etcbc/parallels/tf/c</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 8.4.0</a>, <a target=\"_blank\" href=\"https://github.com/annotation/app-bhsa\" title=\"bhsa TF-app\">app-bhsa</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br><b>Data:</b> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/0_home\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/writing/hebrew.html\" title=\"How TF features represent text\">Character table</a>, <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/0_home\" title=\"BHSA feature documentation\">Feature docs</a><br><b>Features:</b><br><details><summary><b>Parallel Passages</b></summary><b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"~/text-fabric-data/etcbc/parallels/tf/c/crossref.tf\">crossref</a></i></b><br></details><details><summary><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b></summary><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/book.tf\">book</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book@ll\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/book@am.tf\">book@ll</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/chapter\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/chapter.tf\">chapter</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/code\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/code.tf\">code</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/det\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/det.tf\">det</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/domain\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/domain.tf\">domain</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/freq_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/freq_lex.tf\">freq_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/function\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/function.tf\">function</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_cons.tf\">g_cons</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_cons_utf8.tf\">g_cons_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_lex.tf\">g_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_lex_utf8.tf\">g_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_word.tf\">g_word</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/g_word_utf8.tf\">g_word_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gloss\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/gloss.tf\">gloss</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/gn.tf\">gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/label\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/label.tf\">label</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/language\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/language.tf\">language</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/lex.tf\">lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/lex_utf8.tf\">lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ls\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/ls.tf\">ls</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nametype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nametype.tf\">nametype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nme\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nme.tf\">nme</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/nu.tf\">nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/number\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/number.tf\">number</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/otype\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pargr\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pargr.tf\">pargr</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pdp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pdp.tf\">pdp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pfm\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/pfm.tf\">pfm</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs.tf\">prs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_gn\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_gn.tf\">prs_gn</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_nu\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_nu.tf\">prs_nu</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/prs_ps.tf\">prs_ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ps\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/ps.tf\">ps</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere.tf\">qere</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer.tf\">qere_trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_trailer_utf8.tf\">qere_trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/qere_utf8.tf\">qere_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rank_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/rank_lex.tf\">rank_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rela\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/rela.tf\">rela</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/sp\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/sp.tf\">sp</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/st\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/st.tf\">st</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/tab\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/tab.tf\">tab</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/trailer.tf\">trailer</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/trailer_utf8.tf\">trailer_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/txt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/txt.tf\">txt</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/typ\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/typ.tf\">typ</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/uvf\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/uvf.tf\">uvf</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbe\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vbe.tf\">vbe</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vbs.tf\">vbs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/verse\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/verse.tf\">verse</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/voc_lex.tf\">voc_lex</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex_utf8\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/voc_lex_utf8.tf\">voc_lex_utf8</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vs\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vs.tf\">vs</a><br><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vt\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/vt.tf\">vt</a><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/mother\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/mother.tf\">mother</a></i></b><br><b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/oslots\" title=\"~/text-fabric-data/etcbc/bhsa/tf/c/oslots.tf\">oslots</a></i></b><br></details><details><summary><b>Phonetic Transcriptions</b></summary><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/c/phono.tf\">phono</a><br><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/etcbc/phono/tf/c/phono_trailer.tf\">phono_trailer</a><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/server/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 0.1rem;\n",
       "    margin: 0.1rem;\n",
       "    direction: ltr;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1rem 0rem;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".section {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--section);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  0.5rem 0.1rem 0.1rem 0.1rem;\n",
       "    margin: 0.8rem 0.1rem 0.1rem 0.1rem;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 0.2rem;\n",
       "    margin-left: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 0.2rem;\n",
       "    margin-right: 0.1rem;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -1.2rem;\n",
       "    margin-left: 1rem;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3rem;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 0.1rem;\n",
       "    margin-left: 0.1rem;\n",
       "    padding: 0.1rem 0.1rem;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 2rem;\n",
       "\tpadding: 1rem;\n",
       "\tborder: 0.1rem solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--section:            hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         0.15rem;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   0.1rem;\n",
       "  --border-width0:      0.1rem;\n",
       "  --border-width1:      0.15rem;\n",
       "  --border-width2:      0.2rem;\n",
       "  --border-width3:      0.3rem;\n",
       "  --border-width4:      0.25rem;\n",
       "  --border-width-plain: 0.1rem;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 0.2rem ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 0.2rem ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.3rem;\n",
       "  padding: 0.2rem;\n",
       "  margin: 0.2rem;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "from pathlib import Path\n",
    "from tf.app import use\n",
    "from bidict import bidict # bidirectional dictionary\n",
    "\n",
    "# custom modules \n",
    "sys.path.append('..')\n",
    "import tf_tools\n",
    "from gbi_functions import id2ref\n",
    "from positions import PositionsTF\n",
    "\n",
    "# organize pathways\n",
    "PROJ_DIR = Path.home().joinpath('github/CambridgeSemiticsLab/translation_traditions_HB')\n",
    "PRIVATE_DATA = PROJ_DIR.joinpath('data/_private_')\n",
    "GBI_DATA_DIR = PRIVATE_DATA.joinpath('GBI_alignment')\n",
    "VERB_DIR = PRIVATE_DATA.joinpath('verb_data')\n",
    "\n",
    "# load GBI data\n",
    "gbi_niv = json.loads(GBI_DATA_DIR.joinpath('niv84.ot.alignment.json').read_text())\n",
    "gbi_esv = json.loads(GBI_DATA_DIR.joinpath('esv.ot.alignment.json').read_text())\n",
    "\n",
    "# load BHSA / GBI Alignment\n",
    "bhsa2gbi = json.loads(GBI_DATA_DIR.joinpath('bhsa2gbi.json').read_text())\n",
    "\n",
    "# load BHSA data and methods\n",
    "bhsa = use('bhsa')\n",
    "api = bhsa.api\n",
    "F, E, T, L = api.F, api.E, api.T, api.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up some dictionaries for convenient word data access\n",
    "# see 'Dict demos' in next cells for quick intro to the resulting dict structures\n",
    "\n",
    "sources = (('niv', gbi_niv), ('esv', gbi_esv))\n",
    "word_data = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "verse2words = collections.defaultdict(lambda: collections.defaultdict(list)) \n",
    "linkbyid = collections.defaultdict(list) # list of 2-tuples, each containing word IDs\n",
    "id2link = collections.defaultdict(dict) # select a link based on a single ID \n",
    "\n",
    "for name, source in sources:\n",
    "    for verse in source:\n",
    "        \n",
    "        # unpack words for processing\n",
    "        trans_words =  verse['translation']['words']\n",
    "        manu_words = verse['manuscript']['words']\n",
    "        \n",
    "        # map translation word data\n",
    "        for w in trans_words:\n",
    "            ref_tuple = id2ref(w['id'], 'translation')\n",
    "            verse2words[name][ref_tuple].append(w['id'])\n",
    "            word_data[name][w['id']] = w\n",
    "        \n",
    "        # map WLC word data\n",
    "        # arbitrarily use the copy stored under NIV\n",
    "        if name == 'niv':\n",
    "            for w in manu_words:\n",
    "                ref_tuple = id2ref(w['id'])\n",
    "                verse2words['wlc'][ref_tuple].append(w['id'])\n",
    "                word_data['wlc'][w['id']] = w\n",
    "                \n",
    "        # map links to word ids\n",
    "        # the alignment data just contains indices pointing\n",
    "        # to the various lists, so these have to be used to \n",
    "        # identify the specific word in question\n",
    "        for wlc_indices, trans_indices in verse['links']:\n",
    "            wlc_ids = tuple(manu_words[i]['id'] for i in wlc_indices)\n",
    "            trans_ids = tuple(sorted(trans_words[i]['id'] for i in trans_indices))\n",
    "            linkbyid[name].append((wlc_ids, trans_ids))\n",
    "            for wid in wlc_ids:\n",
    "                id2link[name][wid] = trans_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 10010010021,\n",
       " 'altId': 'בָּרָ֣א\\u200e-1',\n",
       " 'text': 'בָּרָ֣א\\u200e',\n",
       " 'strong': 'H1254',\n",
       " 'gloss': 'he created',\n",
       " 'gloss2': '创造',\n",
       " 'lemma': 'ברא_1',\n",
       " 'pos': 'verb',\n",
       " 'morph': 'vqp3ms'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data['wlc'][10010010021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1001001005,\n",
       " 'altId': 'created-1',\n",
       " 'text': 'created',\n",
       " 'transType': 'k',\n",
       " 'isPunc': False,\n",
       " 'isPrimary': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data['niv'][1001001005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1001001001,\n",
       " 1001001002,\n",
       " 1001001003,\n",
       " 1001001004,\n",
       " 1001001005,\n",
       " 1001001006,\n",
       " 1001001007,\n",
       " 1001001008,\n",
       " 1001001009,\n",
       " 1001001010,\n",
       " 1001001011]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verse2words['niv'][('Genesis', 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10010010021,) (1001001005,)\n"
     ]
    }
   ],
   "source": [
    "for wlc_ids, trans_ids in linkbyid['niv']:\n",
    "    if 10010010021 in wlc_ids:\n",
    "        print(wlc_ids, trans_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001001005,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB: similar to above, however\n",
    "# the link is only 1-to-X\n",
    "# so some parts of the left side of the link could be missing\n",
    "id2link['niv'][10010010021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-sample verbs\n",
    "\n",
    "The basis of the dataset is verbs. The jumping-off point is the BHSA syntax data. Thus what\n",
    "we do is assemble the dataset by the BHSA verbs.\n",
    "\n",
    "First we get a one-to-one verb mapping between BHSA and GBI Hebrew (WLC). We can use the GBI hebrew\n",
    "links to select the correct words in the translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main stipulation for the selection is agreement between BHSA and WLC on \n",
    "the classification of a word as a verb (e.g. טוֹב); for BHSA classification, \n",
    "we use a contextual definition (phrase-dependent) which classifies whether \n",
    "the word is behaving as a verb in context (e.g. participles)\n",
    "\n",
    "Since some alignments between BHSA and WLC are `many-to-N` or `N-to-many`, \n",
    "we also filter out any of these non-verbal words. This leaves us with\n",
    "a 1-to-1 alignment, so that 1 verb in BHSA equals 1 verb in WLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4427 verbs do not match requirements\n",
      "68826 selected for building dataset\n"
     ]
    }
   ],
   "source": [
    "# track where BHSA and WLC disagree on the classification of a verb\n",
    "no_match = {\n",
    "    'disagree': [],\n",
    "}\n",
    "\n",
    "verb_bhsa2gbi = {}\n",
    "\n",
    "# find 1-to-1 matches of BHSA and WLC verbs\n",
    "for bhsa_nodes, gbi_ids in bhsa2gbi:\n",
    "    \n",
    "    # filter out non-verbs from the links\n",
    "    bhsa_verbs = [w for w in bhsa_nodes if F.pdp.v(w) == 'verb'] \n",
    "    wlc_verbs = [w for w in gbi_ids if word_data['wlc'][w]['pos'] == 'verb']\n",
    "    data = (T.text(bhsa_nodes), T.sectionFromNode(bhsa_nodes[0]), bhsa_nodes, gbi_ids) # track null matches\n",
    "    \n",
    "    # one case, Jer 51:3, has a double verb mapping caused by \n",
    "    # ידרך ידרך, which BHSA maps to a single word node, and gbi \n",
    "    # keeps as 2 words; we disambig that here and keep only \n",
    "    # first gbi word\n",
    "    if bhsa_verbs and bhsa_verbs[0] == 262780:\n",
    "         wlc_verbs = wlc_verbs[:1]\n",
    "    \n",
    "    # skip non-verbal contexts\n",
    "    if not bhsa_verbs + wlc_verbs:\n",
    "        continue\n",
    "    \n",
    "    # track disagreements between 2 sources\n",
    "    elif (bhsa_verbs and not wlc_verbs) or (wlc_verbs and not bhsa_verbs):\n",
    "        no_match['disagree'].append(data)\n",
    "    \n",
    "    # store result both ways: bhsa 2 wlc, wlc 2 bhsa\n",
    "    elif len(bhsa_verbs) == 1 and len(wlc_verbs) == 1:\n",
    "        \n",
    "        # make a subset selection of verbs\n",
    "        bhsa_verb, wlc_verb = bhsa_verbs[0], wlc_verbs[0]\n",
    "        verb_bhsa2gbi[bhsa_verb] = word_data['wlc'][wlc_verb]\n",
    "        \n",
    "    \n",
    "    # or there's a problem...\n",
    "    else:\n",
    "        raise Exception(f'Misalignment at {data}')\n",
    "        \n",
    "print(sum(len(v) for v in no_match.values()), 'verbs do not match requirements')\n",
    "print(len(verb_bhsa2gbi), 'selected for building dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB that the ~4.4k verbs in disagreement is because we use contextual parts of speech\n",
    "from the BHSA dataset. The GBI dataset does not seem to be as sensitive to context for\n",
    "pos. A large proportion of these cases are participles used as nouns rather than verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('פְּקֻדֵיהֶ֖ם ', ('Numbers', 1, 39), [70153], [40010390011, 40010390012]),\n",
       " ('יֹצֵ֥א ', ('Numbers', 1, 40), [70183], [40010400141]),\n",
       " ('פְּקֻדֵיהֶ֖ם ', ('Numbers', 1, 41), [70185], [40010410011, 40010410012]),\n",
       " ('יֹצֵ֥א ', ('Numbers', 1, 42), [70214], [40010420141]),\n",
       " ('פְּקֻדֵיהֶ֖ם ', ('Numbers', 1, 43), [70216], [40010430011, 40010430012]),\n",
       " ('פְּקֻדִ֡ים ', ('Numbers', 1, 44), [70229], [40010440022]),\n",
       " ('פְּקוּדֵ֥י ', ('Numbers', 1, 45), [70250], [40010450031]),\n",
       " ('פְּקֻדִ֔ים ', ('Numbers', 1, 46), [70271], [40010460032]),\n",
       " ('פְקֻדֵיהֶ֑ם ', ('Numbers', 2, 4), [70477], [40020040022, 40020040023]),\n",
       " ('פְקֻדָ֑יו ', ('Numbers', 2, 6), [70502], [40020060022, 40020060023])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_match['disagree'][500:510]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the dataset for later processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing English with Spacy\n",
    "\n",
    "For understanding the basics of Spacy, see:\n",
    "https://spacy.io/usage/linguistic-features\n",
    "\n",
    "For each verb in the `select_verbs` dictionary, we retrieve its verse text in a\n",
    "given translation. The translated text is parsed by Spacy, which supplies us with\n",
    "a dependency tree, parts of speech, and verb tenses for the English side of things.\n",
    "\n",
    "For Spacy tags used with the model of choice, see:\n",
    "https://github.com/explosion/spacy-models/releases//tag/en_core_web_sm-2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "from spacy.util import filter_spans # filter overlaps; nice tip: https://stackoverflow.com/a/63303480/8351428\n",
    "from spacy.gold import align # align different tokenizations: https://spacy.io/usage/linguistic-features#aligning-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build links between BHSA and English translations via WLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-verbs selected for esv: 66837\n",
      "\tn-verbs unlinked: 1989\n",
      "n-verbs selected for niv: 65455\n",
      "\tn-verbs unlinked: 3371\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "\n",
    "english_verbs = collections.defaultdict(dict)\n",
    "not_linked = collections.defaultdict(list)\n",
    "\n",
    "for trans in ('esv', 'niv'):\n",
    "    \n",
    "    for bhsa_node, wlc_word in verb_bhsa2gbi.items():\n",
    "        try:\n",
    "            english_verbs[trans][bhsa_node] = id2link[trans][wlc_word['id']]\n",
    "        except: \n",
    "            not_linked[trans].append(wlc_word)\n",
    "        \n",
    "    print(f'n-verbs selected for {trans}: {len(english_verbs[trans])}')\n",
    "    print(f'\\tn-verbs unlinked: {len(not_linked[trans])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse English Data within its Verse Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esv has 21264 verses to parse\n",
      "niv has 21251 verses to parse\n"
     ]
    }
   ],
   "source": [
    "# to avoid double-parsing verses, we map\n",
    "# all cases needed to be parsed to a single\n",
    "# parsed verse\n",
    "\n",
    "# for each translation, gather the verse labels\n",
    "# that need to be parsed\n",
    "\n",
    "versestoparse = collections.defaultdict(set)\n",
    "for trans in english_verbs:\n",
    "    for bhsa_node, trans_words in english_verbs[trans].items():\n",
    "        ref = id2ref(trans_words[0], 'translation')\n",
    "        versestoparse[trans].add(ref)\n",
    "        \n",
    "    print(f'{trans} has {len(versestoparse[trans])} verses to parse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply raw Spacy parsing to all relevant verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the Spacy processor as well as some customized attributes\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "Span.set_extension('tam_tag', default='', force=True)\n",
    "Token.set_extension('my_span', default=None, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_verse(verse_tuple, translation):\n",
    "    \"\"\"Parse translation verse with Spacy.\"\"\"\n",
    "    word_ids = verse2words[translation][verse_tuple]\n",
    "    words = [word_data[translation][w] for w in word_ids]\n",
    "    text = ' '.join(w['text'] for w in words)\n",
    "    parsed_doc = nlp(text) # magic happens here\n",
    "    return parsed_doc\n",
    "\n",
    "def parse_verses(transdict):\n",
    "    \"\"\"Iterate through all verses and parse them.\n",
    "    \n",
    "    Args:\n",
    "        versedict: dict with structure of e.g. {'niv': set(('Genesis', 1, 1)...}}\n",
    "    Returns:\n",
    "        dict w/ structure of e.g. {'niv': {('Genesis', 1, 1): Spacy.Doc}}\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed_verses = collections.defaultdict(dict)\n",
    "    \n",
    "    bhsa.indent(0, reset=True)\n",
    "    bhsa.info(f'Parsing translations...')\n",
    "    \n",
    "    for translation, verse_set in transdict.items():\n",
    "    \n",
    "        # it takes a long time so we time it\n",
    "        bhsa.indent(1, reset=True)\n",
    "        bhsa.info(f'Beginning {translation}...')\n",
    "        bhsa.indent(2, reset=True)\n",
    "    \n",
    "        # parse the verse and put Spacy.Doc in a dict\n",
    "        for i, ref_tuple in enumerate(verse_set):\n",
    "            \n",
    "            if i % 5000 == 0 and i != 0:\n",
    "                bhsa.info(f'done with verse {i}')\n",
    "                \n",
    "            parsed_verses[translation][ref_tuple] = parse_verse(ref_tuple, translation)\n",
    "            \n",
    "        bhsa.indent(1)\n",
    "        bhsa.info('done!')\n",
    "    \n",
    "    return parsed_verses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute the parser for all verses. This will take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle here to run fresh parsings\n",
    "parsed_verses_file = PRIVATE_DATA.joinpath('parsings/parsed_verses.pickle')\n",
    "if False:\n",
    "    parsed_verses = parse_verses(versestoparse)\n",
    "    with open(parsed_verses_file, 'wb') as outfile:\n",
    "        pickle.dump(parsed_verses, outfile)\n",
    "else:\n",
    "    with open(parsed_verses_file, 'rb') as infile:\n",
    "        parsed_verses = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "They said to each other , “ Come , let’s make bricks and bake them thoroughly . ” They used brick instead of stone , and tar for mortar ."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_verses['niv'][('Genesis', 11, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Matcher rules for advanced TAM tags\n",
    "\n",
    "Spacy parses raw strings into tags and dependencies. For verbs we are particularly\n",
    "interested in tense, aspect, and modality (TAM). The default tags are not very informative with \n",
    "regard to TAM. But we can also achieve these labels ourselves by adding some additional rules.\n",
    "\n",
    "We will use Spacy's Matcher class for this, alongside the parser:\n",
    "\n",
    "https://spacy.io/usage/rule-based-matching\n",
    "\n",
    "To-do list of primary English tense constructions, curated from:\n",
    "\n",
    "https://en.wikipedia.org/wiki/English_verbs#Expressing_tenses,_aspects_and_moods\n",
    "\n",
    "```\n",
    "simple present            writes\n",
    "simple past               wrote\n",
    "present progressive       is writing\n",
    "past progressive          was writing\n",
    "present perfect           has written\n",
    "past perfect              had written\n",
    "present perf. progress.   has been writing\n",
    "past perf. progress.      had been writing\n",
    "future                    will write\n",
    "future perfect            will have written\n",
    "future perf. progress.    will have been writing\n",
    "```\n",
    "\n",
    "secondary constructions:\n",
    "\n",
    "```\n",
    "imperative               write\n",
    "future-in-past           would write\n",
    "do-support               does write\n",
    "be-going-to future       is going to write\n",
    "```\n",
    "\n",
    "Later on, we can consider dividing these constructions up into 3 columns -- 1 each for \n",
    "tense, aspect, and modality. If a construction contributes to one of these categories,\n",
    "the column gets filled. Otherwise it is left empty. \n",
    "\n",
    "```\n",
    "\"has been writing\"\n",
    "\n",
    "tense           aspect            modality\n",
    "-----           ------             ------\n",
    "past      perfect progressive\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set of rules to match tense-aspect-modality construtions in English\n",
    "# overlapping results will be filtered out and the longest matching span\n",
    "# will be kept in its place\n",
    "\n",
    "# these patterns can be inserted between verb auxiliaries\n",
    "# and their heads to represent any number of interrupting\n",
    "# adverbial modifiers\n",
    "advb_pronouns = {'TAG': {'IN':['RB', 'PRP']}, 'OP': '*'}\n",
    "advbs = {'TAG': {'IN':['RB']}, 'OP': '*'}\n",
    "non_verbs = {'TAG': {'NOT_IN':['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']}, 'OP': '*'}\n",
    "modal_set = ['let', 'may', 'shall', 'must', 'could', 'can']\n",
    "    \n",
    "tam_rules = [\n",
    "    (\n",
    "        'PRES (PRES..IND)', \n",
    "        [\n",
    "            {'TAG':{'IN':['VBZ', 'VBP']}, 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PRES PROG (PRES.PROG.IND)', \n",
    "        [\n",
    "            {'TAG': {'IN':['VBZ', 'VBP']}, 'LEMMA':'be'},\n",
    "            advb_pronouns,\n",
    "            {'TAG':'VBG', 'LEMMA': {'NOT_IN':['go']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PRES PERF (PRES.PERF.IND)',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBZ', 'VBP']}, 'LEMMA': {'REGEX': 'have'}},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PRES PERF PROG (PRES.PERF_PROG.IND)',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBZ', 'VBP']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'LEMMA': 'be'},\n",
    "            {'TAG': 'VBG'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PAST (PAST..IND)',\n",
    "        [\n",
    "            {'TAG': 'VBD', 'DEP': {'NOT_IN':['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PAST PERF (PAST.PERF.IND)',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBD']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PAST PERF (PAST.PERF.IND)', # with 'did'\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBD']}, 'LEMMA': 'do', 'DEP': 'aux'},\n",
    "            non_verbs,\n",
    "            {'TAG': 'VB'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PAST PERF PROG (past.PERF_PROG.IND)',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBD']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'LEMMA': 'be'},\n",
    "            {'TAG': 'VBG'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'FUT PERF (FUT.PERF.IND)',\n",
    "        [\n",
    "            {'TAG': 'MD', 'LEMMA': 'will'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': {'IN': ['VB']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'FUT PERF PROG (FUT.PERF_PROG.IND)',\n",
    "        [\n",
    "            {'TAG': 'MD', 'LEMMA': 'will'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': {'IN': ['VB']}, 'LEMMA': 'have'},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBN', 'LEMMA': 'be'},\n",
    "            {'TAG': 'VBG'},\n",
    "        ]\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        'PAST PROG (PAST.PROG.IND)',\n",
    "        [\n",
    "            {'TAG':'VBD', 'LEMMA': {'IN': ['be', 'keep']}},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBG'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'FUT (FUT..IND)',\n",
    "        [\n",
    "            {'TAG': 'MD', 'LEMMA': {'REGEX':'[wW]ill'}, 'DEP': 'aux'},\n",
    "            non_verbs,\n",
    "            {'TAG': 'VB', 'DEP': {'NOT_IN': ['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'FUT-IN-PAST (PAST..SUBJ)', # habitual?\n",
    "        [\n",
    "            {'LOWER': 'would', 'DEP': {'IN': ['aux']}},\n",
    "            advb_pronouns,\n",
    "            {'TAG':'VB'}\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'DO PRES (PRES..IND)',\n",
    "        [\n",
    "            {'TAG': {'IN': ['VBZ', 'VBP']}, 'LEMMA': 'do', 'LOWER': {'NOT_IN':['do']}},\n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VB'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'GOING TO (FUT..MOD)', # going to\n",
    "        [\n",
    "            {'TAG': {'IN':['VBZ', 'VBP']}, 'LEMMA':'be'}, \n",
    "            advb_pronouns,\n",
    "            {'TAG': 'VBG', 'LEMMA': 'go'},\n",
    "            {'TAG': 'TO'},\n",
    "            {'TAG': 'VB'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'MOD (PRES..MOD)',\n",
    "        [\n",
    "            {'TAG': {'IN':['VB', 'MD']}, 'LEMMA': {'IN':modal_set}},\n",
    "            {'TAG': {'NOT_IN':['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']}, 'OP': '*'},\n",
    "            {'TAG': 'VB'},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PRES PART (PRES..)', \n",
    "        [\n",
    "            {'TAG': 'VBG', 'DEP': {'NOT_IN':['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'PRES PART (PRES..)',\n",
    "        [\n",
    "            {'TAG': {'IN': ['JJ']}, 'LOWER':{'REGEX':'^.+ing$'}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'TO INF (..)',\n",
    "        [\n",
    "            {'LOWER': 'to'},\n",
    "            {'TAG':'VB'}\n",
    "        ]\n",
    "    ),\n",
    "    # the imperative in English consists of the base\n",
    "    # form of a verb; but this form can of course\n",
    "    # be used within a number of verb constructions\n",
    "    (\n",
    "        'IMPV (PRES..IMPV)',\n",
    "        [\n",
    "            {'TAG': 'VB', 'DEP':{'NOT_IN':['aux']}},\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        'IMPV (PRES..IMPV)',\n",
    "        [\n",
    "            {'TAG': 'VB', 'LEMMA': {'REGEX':'[dD]o'}},\n",
    "            {'LEMMA': 'not'},\n",
    "            {'TAG': 'VB'}\n",
    "        ]\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "def test_span_impv(span):\n",
    "    \"\"\"Test whether span truly contains an imperative\"\"\"\n",
    "    for token in span:\n",
    "        if test_impv(token):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_span(span):\n",
    "    \"\"\"Double checks matches and adjusts as needed\"\"\"\n",
    "    if span._.tam_tag == 'imperative':\n",
    "        if not test_span_impv(span):\n",
    "            span._.tam_tag = '' # remove the label\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add all rules to the matcher object\n",
    "for tag, rules in tam_rules:\n",
    "    matcher.add(tag, None, rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Testing area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Gen 19:9, tagged as imperative when it's not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others:\n",
    "* gen 20:7, \"if you do not return her\" -- should in general treat \"do\" different\n",
    "* lots of futures get inheritance when they shouldn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nlp2('Did you do that?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Did"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = test[0]\n",
    "\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        print(i, token)\n",
    "        print('  ', token.lemma_)\n",
    "        print('  ', token.pos_)\n",
    "        print('  ', token.tag_)\n",
    "        print('  ', token.dep_)\n",
    "        print('  ', token.shape_)\n",
    "        \n",
    "#display_tokens(parsed_verses['niv'][('Genesis', 1, 24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_impv(token, debug=False):\n",
    "    \"\"\"Test whether a verb is an imperative.\"\"\"\n",
    "    ancestors = list(token.ancestors)\n",
    "    subtree = list(token.subtree)\n",
    "    ancest_tags = set(t.tag_ for t in ancestors if t.tag_.startswith('VB'))\n",
    "    ancest_lemmas = set(t.lemma_ for t in ancestors)\n",
    "    subtree_deps = set(t.dep_ for t in subtree)\n",
    "    head_deps = set(t.dep_ for t in token.head.children)\n",
    "    ancest_lemmas = set(t.lemma_ for t in ancestors)\n",
    "    clause_pos = subtree.index(token)\n",
    "    modal_set = {\n",
    "        'let', 'may', \n",
    "        'shall', 'must', 'can'\n",
    "    }\n",
    "    auxs = {'aux', 'auxpass'}\n",
    "    \n",
    "    ancestor_rules = [\n",
    "        ancest_tags.issubset({'VB'}),\n",
    "        token.dep_ != 'conj',\n",
    "    ]\n",
    "    if ancestors:\n",
    "        ancestor_rules.append(test_impv(token.head))\n",
    "    position_rules = [\n",
    "        clause_pos == 0,\n",
    "        not ancestors,\n",
    "    ]\n",
    "    \n",
    "    # check for boundary\n",
    "    rules = [\n",
    "        token.tag_ == 'VB',\n",
    "        token.dep_ not in auxs,\n",
    "        any(ancestor_rules),\n",
    "        not ancest_lemmas & modal_set,\n",
    "        not head_deps & auxs,\n",
    "        any(position_rules),\n",
    "        token.lemma_ not in modal_set,\n",
    "    ]\n",
    "    # specify that word must occur at \n",
    "    # a major boundary\n",
    "    if token.i != 0:\n",
    "        pre_t = token.doc[token.i-1]\n",
    "        punct_rules = any([\n",
    "            pre_t.is_punct,\n",
    "            pre_t.lemma_ == 'and',\n",
    "            (pre_t.is_title and pre_t.tag_ == 'RB'),\n",
    "            pre_t.lemma == 'please',\n",
    "        ])\n",
    "        rules.append(punct_rules)\n",
    "    else:\n",
    "        rules.append(token.is_title)\n",
    "    \n",
    "    \n",
    "    if debug:\n",
    "        print(rules)\n",
    "        print('anc rules:', ancestor_rules)\n",
    "        print('pos rules:', position_rules)\n",
    "    \n",
    "    if all(rules):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“ Do not keep talking so proudly or let your mouth speak such arrogance , for the LORD is a God who knows , and by him deeds are weighed .\n",
      "\n",
      "0 “  1 Do  2 not  3 keep  4 talking  5 so  6 proudly  7 or  8 let  9 your  10 mouth  11 speak  12 such  13 arrogance  14 ,  15 for  16 the  17 LORD  18 is  19 a  20 God  21 who  22 knows  23 ,  24 and  25 by  26 him  27 deeds  28 are  29 weighed  30 .  "
     ]
    }
   ],
   "source": [
    "test_parse = parsed_verses['niv'][('1_Samuel', 2, 3)]\n",
    "# for token in test_parse:\n",
    "#     is_impv = test_impv(token)\n",
    "#     print(token, f'\\tis impv: {is_impv}')\n",
    "    \n",
    "#display_tokens(test_parse)\n",
    "\n",
    "print(test_parse)\n",
    "print()\n",
    "for i,token in enumerate(test_parse):\n",
    "    print(i, token, end='  ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Do"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = test_parse[1]\n",
    "\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ex.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VBP'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aux'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[keep]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ex.ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VB']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.tag_ for t in ex.ancestors if t.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ex.conjuncts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Do]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ex.subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keep"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, True, True, False, True, True, True]\n",
      "anc rules: [True, True, False]\n",
      "pos rules: [True, False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_impv(ex, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'open clausal complement'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('xcomp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esv n-spans: 21210\n",
      "niv n-spans: 21184\n"
     ]
    }
   ],
   "source": [
    "def attach_span(spans):\n",
    "    \"\"\"Connect a token to its span explicitly.\"\"\"\n",
    "    for span in spans:\n",
    "        for token in span:\n",
    "            token._.my_span = span\n",
    "\n",
    "# TODO: consider how to do this recursively\n",
    "# TODO: clean up fugly code\n",
    "def endow_tam(spans):\n",
    "    \"\"\"Pass down a TAM category from a head to its conjuncts.\"\"\"\n",
    "    for span in spans:\n",
    "        for token in span:\n",
    "            if token.tag_ != 'VB':\n",
    "                continue\n",
    "            conjuncts = token.conjuncts\n",
    "            for conj in conjuncts:\n",
    "                 # apply to non-modified verbal stems\n",
    "                if (cspan := conj._.my_span) and conj.tag_ == 'VB':\n",
    "                    if len(list(cspan)) == 1:               \n",
    "                        conj._.my_span._.tam_tag = span._.tam_tag\n",
    "    \n",
    "# for every verse isolate the set of relevant spans\n",
    "# which match the TAM rules and map to verses\n",
    "verse2spans = collections.defaultdict(dict)\n",
    "for trans, ref_tuples in parsed_verses.items():\n",
    "    for ref_tuple, spacy_doc in ref_tuples.items():\n",
    "        matches = matcher(spacy_doc)\n",
    "        \n",
    "        # retrieve Spacy Span objects\n",
    "        # and give them TAM tags\n",
    "        spans = []\n",
    "        for m_id, start, end in matches:\n",
    "            span = spacy_doc[start:end]\n",
    "            span._.tam_tag = nlp.vocab.strings[m_id]\n",
    "            check_span(span)\n",
    "            spans.append(span)\n",
    "        \n",
    "        # filter out overlapping spans and keep \n",
    "        # only the longest strings\n",
    "        filtered_spans = filter_spans(spans)\n",
    "        #attach_span(filtered_spans)\n",
    "        #endow_tam(filtered_spans)\n",
    "        \n",
    "        # save positive matches; unmatched verses will\n",
    "        # be recognized later\n",
    "        if filtered_spans:\n",
    "            verse2spans[trans][ref_tuple] = filtered_spans\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    print(f'{trans} n-spans: {len(verse2spans[trans])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link spans to verbs\n",
    "\n",
    "We will now attempt to re-link the spans with the verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = [str(t) for t in parsed_verses['niv'][('Genesis', 11, 3)]]\n",
    "# gbi_words = [word_data['niv'][w]['text'] for w in verse2words['niv'][('Genesis', 11, 3)]]\n",
    "# gbi_ids = verse2words['niv'][('Genesis', 11, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, token in enumerate(parsed_verses['niv'][('Genesis', 11, 3)]):\n",
    "#     gid = gbi_ids[a2b_multi.get(i, a2b[i])]\n",
    "#     #print(token, gid, word_data['niv'][gid]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s matching spans...\n",
      "    21s done with matches\n"
     ]
    }
   ],
   "source": [
    "def trans_to_span(para_words, spans, \n",
    "                  verse_words, aligner):\n",
    "    \"\"\"Match given words with its TAM span.\n",
    "    \n",
    "    A match is an overlap of known parallel words and \n",
    "    a span of matched words, based on overlapping GBI ids.\n",
    "    Thus all Spacy token indicies are converted to GBI indicies\n",
    "    and used to lookup the corresponding GBI ids for set comparison.\n",
    "    \n",
    "    Args:\n",
    "        para_words: list of gbi word ids for a known parallel alignment \n",
    "        spans: list of Spacy Span objects from the Matcher, with tam_tag attributes\n",
    "        verse_words: list of gbi ids within a verse; is indexed with remapped indices\n",
    "            from the Span tokens, which have attributes `start` and `end` which\n",
    "            correspond with their index in the Spacy doc (verse). Those indices are\n",
    "            remapped to their GBI positions with the `aligner`.        \n",
    "        aligner: remaps Spacy indicies to GBI indicies for tokens\n",
    "    \"\"\"\n",
    "    for span in spans:\n",
    "        start, end = aligner(span.start), aligner(span.end-1) \n",
    "        end += 1 # -1 above to avoid IndexError since end might be +1 longer than end for index slicing\n",
    "        span_words = set(verse_words[start:end])\n",
    "        if set(para_words) & span_words:\n",
    "            return span\n",
    "\n",
    "bhsa.indent(0, reset=True)\n",
    "bhsa.info('matching spans...')\n",
    "        \n",
    "verse_inspect = collections.defaultdict(lambda: collections.defaultdict(str)) # for exporting an inspection document\n",
    "bhsa2eng = collections.defaultdict(dict)\n",
    "\n",
    "for trans, bhsa_nodes in english_verbs.items():\n",
    "    \n",
    "    for bhsa_node, para_words in bhsa_nodes.items():\n",
    "\n",
    "        inspect = '' # for debugging and inspection\n",
    "        \n",
    "        # get GBI-side data\n",
    "        verse_ref = id2ref(para_words[0], 'translation')        \n",
    "        para_text = ' '.join(word_data[trans][w]['text'] for w in para_words)\n",
    "        verse_words = verse2words[trans][verse_ref]\n",
    "        verse_tokens = [word_data[trans][w]['text'] for w in verse_words]\n",
    "        \n",
    "        # get Spacy-side data\n",
    "        verse_parsing = parsed_verses[trans][verse_ref]\n",
    "        spacy_tokens = [str(t) for t in verse_parsing]\n",
    "        \n",
    "        # map Spacy tokens back to GBI tokens using indicies\n",
    "        # Spacy tokenizes words with apostrophes differently (for e.g. `he'll` == `he` + `'ll`)\n",
    "        # They can be re-aligned: https://spacy.io/usage/linguistic-features#aligning-tokenization\n",
    "        cost, a2b, b2a, a2b_multi, b2a_multi = align(spacy_tokens, verse_tokens) # alignment of indicies here\n",
    "        aligner = lambda i: a2b_multi.get(i, a2b[i]) # returns 1-to-1 or many-to-1 aligned index\n",
    "        \n",
    "        # try to retrieve span links with advanced TAM tags\n",
    "        spans = verse2spans[trans].get(verse_ref, [])\n",
    "        span_match = trans_to_span(para_words, spans, verse_words, aligner) or '' # search for overlapping GBI id sets\n",
    "        if span_match:\n",
    "            tam_tag = span_match._.tam_tag\n",
    "        else:\n",
    "            tam_tag = ''\n",
    "        \n",
    "        # retrieve basic parsings\n",
    "        raw_tokens = []\n",
    "        for i, token in enumerate(verse_parsing):\n",
    "            if verse_words[aligner(i)] in para_words:\n",
    "                raw_tokens.append(token)\n",
    "                \n",
    "        vb_tokens = [t for t in raw_tokens if t.tag_.startswith('VB')]\n",
    "            \n",
    "        # save the data\n",
    "        data = {\n",
    "            'words': para_text,\n",
    "            'tags': '|'.join(t.tag_ for t in raw_tokens),\n",
    "            'vb_tags': '|'.join(t.tag_ for t in vb_tokens),\n",
    "            'TAM_cx': tam_tag,\n",
    "            'TAM_span': f'{span_match}',\n",
    "        }\n",
    "        \n",
    "        bhsa2eng[trans][bhsa_node] = data\n",
    "            \n",
    "        # add strings to inspection file\n",
    "        if span_match and span_match._.tam_tag:\n",
    "            verse_inspect[trans][verse_ref] += f'\\tMATCH: {para_text}\\n'\n",
    "            verse_inspect[trans][verse_ref] += f'\\t\\t{span_match} -> {span_match._.tam_tag}\\n'\n",
    "        else:\n",
    "            verse_inspect[trans][verse_ref] += f'\\tMISS: {para_text}\\n'\n",
    "            \n",
    "bhsa.info('done with matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "for trans, trans_data  in bhsa2eng.items():\n",
    "    trans_file = VERB_DIR.joinpath(f'bhsa2{trans}.json')\n",
    "    with open(trans_file, 'w') as outfile:\n",
    "        json.dump(trans_data, outfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # export inspection file\n",
    "    write = ''\n",
    "    inspect_file = PRIVATE_DATA.joinpath(f'debugging/{trans}_inspect.txt')\n",
    "    for verse, message in verse_inspect[trans].items():\n",
    "        write += '{} {}:{}'.format(*verse) + '\\n'\n",
    "        write += str(parsed_verses[trans][verse]) + '\\n'\n",
    "        write += message\n",
    "        write += '\\n'\n",
    "    inspect_file.write_text(write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the bhsa2wlc dataset\n",
    "wlc_verbdataset_path = VERB_DIR.joinpath('bhsa2wlc.json') \n",
    "with open(wlc_verbdataset_path, 'w') as outfile:\n",
    "    json.dump(verb_bhsa2gbi, outfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save these resulting dicts for later convenient use\n",
    "\n",
    "# save_files = [\n",
    "#     ('word_data', word_data),\n",
    "#     ('verse2words', {k:{tuple(k2):v2 for k2,v2 in v.items()} for k,v in verse2words.items()}),\n",
    "#     ('id_links', linkbyid),\n",
    "# ]\n",
    "\n",
    "# for filename, data in save_files:\n",
    "#     filepath = GBI_DATA_DIR.joinpath(filename+'.json')\n",
    "#     with open(filepath, 'w') as outfile:\n",
    "#         json.dump(data, outfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export the dataset\n",
    "# dataset_path = PROJ_DIR.joinpath('data/_private_/translation_dataset.csv')\n",
    "\n",
    "# data_df.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "Scratch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write = ''\n",
    "# for verse, cases in verse_sees.items():\n",
    "#     parsed_verse = parsed_verses['niv'][verse]\n",
    "#     ref_str = '{} {}:{}'.format(*verse)\n",
    "#     write += ref_str + '\\n'\n",
    "#     write += str(parsed_verse) + '\\n'\n",
    "#     for case in cases:\n",
    "#         write += case\n",
    "#     write += '\\n'\n",
    "    \n",
    "# Path('see_cases.txt').write_text(write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
